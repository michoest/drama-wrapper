{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Learning optimal restrictions in a continuous-action game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook corresponds to Section 5.3 of the paper \"Grams & Oesterle (forthcoming). _DRAMA at the PettingZoo: Dynamically Restricted Action Spaces for Multi-Agent Reinforcement Learning Frameworks_.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(f'{os.getcwd()}/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Space\n",
    "\n",
    "from src.restrictions import DiscreteVectorRestriction\n",
    "from src.wrapper import RestrictionWrapper\n",
    "from src.restrictors import Restrictor, RestrictorActionSpace, DiscreteVectorActionSpace\n",
    "from src.utils import flatdim, flatten, unflatten\n",
    "\n",
    "from examples.utils import play, ReplayBuffer\n",
    "from examples.traffic.env import TrafficEnvironment\n",
    "from examples.traffic.agent import TrafficAgent\n",
    "from examples.traffic.restrictor import TrafficRestrictor\n",
    "\n",
    "from examples.traffic.utils import create_graph, analyze_graph, edge_path_to_node_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_graph([\n",
    "    ((0, 1), (0, 8, 1)), \n",
    "    ((0, 2), (11, 0, 0)), \n",
    "    ((1, 2), (1, 0, 0)), \n",
    "    ((1, 3), (11, 0, 0)), \n",
    "    ((2, 3), (0, 8, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_agent_routes = [(0, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "number_of_nodes = graph.number_of_nodes()\n",
    "number_of_edges = graph.number_of_edges()\n",
    "\n",
    "edge_list = list(graph.edges)\n",
    "edge_indices = {e: i for i, e in enumerate(edge_list)}\n",
    "edge_latencies = {i: graph[s][t][\"latency\"] for i, [s, t] in enumerate(edge_list)}\n",
    "\n",
    "minimum_node_set = set(sum(possible_agent_routes, tuple()))\n",
    "\n",
    "valid_edge_restrictions = []\n",
    "for allowed_edges in powerset(range(number_of_edges)):\n",
    "    subgraph = graph.edge_subgraph(edge_list[i] for i in allowed_edges)\n",
    "    if minimum_node_set.issubset(subgraph.nodes) and all(nx.has_path(subgraph, s, t) for s, t in possible_agent_routes):\n",
    "        valid_edge_restrictions.append(set(allowed_edges))\n",
    "\n",
    "route_list = [tuple(edge_indices[e] for e in path)\n",
    "        for s, t in possible_agent_routes\n",
    "        for path in nx.all_simple_edge_paths(graph, s, t)\n",
    "]\n",
    "number_of_routes = len(route_list)\n",
    "\n",
    "valid_route_restrictions = [np.array([set(route).issubset(edge_restriction) for route in route_list]) for edge_restriction in valid_edge_restrictions]\n",
    "\n",
    "source_target_map = [(s, t) for s, t in possible_agent_routes for _ in nx.all_simple_edge_paths(graph, s, t)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Without Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_agents = 2\n",
    "\n",
    "edge_list, edge_indices, edge_latencies, routes, route_list, route_indices = analyze_graph(graph)\n",
    "\n",
    "agents = {f'agent_{i}': TrafficAgent(routes, route_indices, edge_indices) for i in range (number_of_agents)}\n",
    "env = TrafficEnvironment(graph, list(agents), possible_routes, number_of_steps=100)\n",
    "policies = {id: agent.act for id, agent in agents.items()}\n",
    "\n",
    "trajectory = play(env, policies, max_iter=50, verbose=False, record_trajectory=True, render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.groupby('agent')['reward'].plot(legend=True, xlabel='Time step', ylabel='Reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = trajectory.groupby('agent')['action'].plot(style='.', legend=True)\n",
    "ax.set_yticks(list(route_indices.values()), [edge_path_to_node_path(route, edge_list) for route in route_indices.keys()]);\n",
    "ax.set_ylabel('Route taken')\n",
    "ax.set_xlabel('Time step')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_agents = 2\n",
    "\n",
    "edge_list, edge_indices, edge_latencies, routes, route_list, route_indices = analyze_graph(graph)\n",
    "number_of_edges = graph.number_of_edges()\n",
    "\n",
    "agents = {f'agent_{i}': TrafficAgent(routes, route_indices, edge_indices) for i in range (number_of_agents)}\n",
    "env = TrafficEnvironment(graph, list(agents), possible_routes, number_of_steps=100)\n",
    "\n",
    "restrictor = TrafficRestrictor(Box(0, np.inf, shape=(number_of_edges, )), DiscreteVectorActionSpace(Discrete(len(routes))))\n",
    "wrapper = RestrictionWrapper(env, restrictor, restrictor_reward_fns={'restrictor_0': lambda env, rewards: rewards[env.agent_selection]})\n",
    "\n",
    "policies = {**{id: agent.act for id, agent in agents.items()}, 'restrictor_0': restrictor.act}\n",
    "\n",
    "trajectory = play(wrapper, policies, max_iter=50, verbose=False, record_trajectory=True, render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.groupby('agent')['reward'].plot(legend=True, xlabel='Time step', ylabel='Reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = trajectory[trajectory['agent'] != 'restrictor_0'].groupby('agent')['action'].plot(style='.', legend=True)\n",
    "ax.set_yticks(list(route_indices.values()), [edge_path_to_node_path(route, edge_list) for route in route_indices.keys()]);\n",
    "ax.set_ylabel('Route taken')\n",
    "ax.set_xlabel('Time step')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With self-learning restrictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_agents = 2\n",
    "\n",
    "total_timesteps = 100_000\n",
    "\n",
    "number_of_edges = graph.number_of_edges()\n",
    "\n",
    "agents = {f'agent_{i}': TrafficAgent(route_list, source_target_map) for i in range (number_of_agents)}\n",
    "restrictor = TrafficRestrictor(number_of_edges, number_of_routes,\n",
    "                               valid_route_restrictions, total_timesteps=total_timesteps)\n",
    "\n",
    "env = TrafficEnvironment(graph, list(agents), possible_agent_routes, number_of_routes, edge_latencies, route_list, number_of_steps=100)\n",
    "\n",
    "env = RestrictionWrapper(env, restrictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(columns=['episode', 'episode_step', 'agent', 'observation', 'reward', 'action'], index=(range(total_timesteps)))\n",
    "replay_buffer = ReplayBuffer(state_dim=flatdim(restrictor.observation_space), action_dim=flatdim(restrictor.action_space))\n",
    "\n",
    "# Do not render during training\n",
    "env.unwrapped.render_mode = None\n",
    "\n",
    "current_timestep = 0\n",
    "current_episode = 0\n",
    "t = tqdm(total=total_timesteps)\n",
    "\n",
    "while current_timestep < total_timesteps:\n",
    "    env.reset()\n",
    "    current_episode += 1\n",
    "    current_episode_timestep = 0\n",
    "    previous_restrictor_observation = None\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if agent == 'restrictor_0':\n",
    "            if previous_restrictor_observation is not None:\n",
    "                restrictor.learn(previous_restrictor_observation, previous_restrictor_action, observation, reward, termination or truncation)\n",
    "\n",
    "            action = restrictor.act(observation)\n",
    "\n",
    "            previous_restrictor_observation = observation\n",
    "            previous_restrictor_action = action\n",
    "        else:\n",
    "            action = agents[agent].act(observation)\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "\n",
    "        # print(f'{agent=}, {observation=}, {reward=}, {action=}')\n",
    "\n",
    "            history.loc[current_timestep] = pd.Series({'episode': current_episode, \n",
    "                                               'episode_step': current_episode_timestep, \n",
    "                                               'agent': agent,\n",
    "                                               'observation': observation, \n",
    "                                               'reward': reward, \n",
    "                                               'action': action}\n",
    "                                               )\n",
    "            \n",
    "            current_timestep += 1\n",
    "            current_episode_timestep += 1\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictor_history = history[history.agent == 'restrictor_0']\n",
    "restrictor_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictor_actions = restrictor_history['action'].astype(int)\n",
    "restrictor_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(range(15))\n",
    "\n",
    "restrictor_action_counts = np.empty((len(restrictor_actions), len(actions)))\n",
    "counts = np.zeros(len(actions))\n",
    "\n",
    "for i, action in enumerate(restrictor_actions):\n",
    "    counts[action] += 1\n",
    "    restrictor_action_counts[i] = counts\n",
    "\n",
    "pd.DataFrame(restrictor_action_counts, columns=map(str, valid_edge_restrictions)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = restrictor_actions.astype(int).plot(style='.', ms=0.5)\n",
    "ax.set_yticks(range(len(valid_edge_restrictions)), valid_edge_restrictions);\n",
    "ax.set_ylabel('Allowed edges')\n",
    "ax.set_xlabel('Time step');\n",
    "\n",
    "ax.get_figure().savefig('result.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_edge_restrictions[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictor_actions.iloc[388]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[history.agent == 'restrictor_0'].reward.rolling(1000).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_begin = history[:10000]\n",
    "history_end = history[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_begin.groupby('agent')['reward'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_end.groupby('agent')['reward'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = history_end[history_end['agent'] != 'restrictor_0'].groupby('agent')['action'].plot(style='.', legend=True)\n",
    "ax.set_yticks(range(len(route_list)), [edge_path_to_node_path(route, edge_list) for route in route_list]);\n",
    "ax.set_ylabel('Route taken')\n",
    "ax.set_xlabel('Time step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = history[history['agent'] == 'restrictor_0']['action'].astype(int).plot(style='.', legend=True)\n",
    "ax.set_yticks(range(len(valid_edge_restrictions)), valid_edge_restrictions);\n",
    "ax.set_ylabel('Route restriction')\n",
    "ax.set_xlabel('Time step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'E': [0, 0, 0, 0, 1, 1], 'T': [0, 0, 1, 1, 0, 0], 'A': [0, 1, 0, 1, 0, 1], 'R': [5, 4, 4, 3, 3, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = [(e, t) for e, t in zip(df['E'], df['T'])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((df[df.A == a]['R'] for a in df.A.unique()), axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df[df.A == a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.index = [(e, t) for e, t in zip(df0['E'], df0['T'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
