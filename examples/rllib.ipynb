{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_compile is deprecated, use jit_compile instead\n",
      "WARNING:tensorflow:From /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:61: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n",
      "/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/numpy/dtype.py:82: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  bool = np.bool  # pylint: disable=redefined-builtin\n",
      "/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/numpy/dtype.py:112: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  string = getattr(np, 'str', getattr(np, 'string', None))\n",
      "/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:61: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n",
      "/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/mcmc/sample_halton_sequence.py:373: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sieve = np.ones(n // 3 + (n % 6 == 2), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gymnasium.spaces import Dict, Box\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from envs.rllib import DummyEnvironment, NavigationEnvironment\n",
    "from examples.agents.policies import MPSTD3Policy\n",
    "from examples.restrictors.navigation_restrictor import NavigationRestrictor\n",
    "from src.spaces.interval_union import IntervalUnion\n",
    "from src.utils import test_environment\n",
    "from src.wrappers.rllib import UniformlyRestrictedEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtest_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDummyEnvironment\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Institute for Enterprise Systems/Paper/hicss-2024/src/utils.py:18\u001B[0m, in \u001B[0;36mtest_environment\u001B[0;34m(env, restrictor, env_config, num_steps, num_episodes, individual)\u001B[0m\n\u001B[1;32m     16\u001B[0m     env_config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misclass(env):\n\u001B[0;32m---> 18\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTesting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(env)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     22\u001B[0m obs, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[0;32m~/Documents/Institute for Enterprise Systems/Paper/hicss-2024/examples/envs/rllib.py:26\u001B[0m, in \u001B[0;36mDummyEnvironment.__init__\u001B[0;34m(self, env_config)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magents \u001B[38;5;241m=\u001B[39m env_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magents\u001B[39m\u001B[38;5;124m'\u001B[39m, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnumber_of_steps \u001B[38;5;241m=\u001B[39m env_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber_of_steps\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m env_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobservation_space\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m---> 26\u001B[0m                                         \u001B[43mgym\u001B[49m\u001B[38;5;241m.\u001B[39mspaces\u001B[38;5;241m.\u001B[39mBox(low\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, high\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magents),)))\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space \u001B[38;5;241m=\u001B[39m env_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maction_space\u001B[39m\u001B[38;5;124m'\u001B[39m, gym\u001B[38;5;241m.\u001B[39mspaces\u001B[38;5;241m.\u001B[39mDiscrete(\u001B[38;5;241m3\u001B[39m))\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "test_environment(DummyEnvironment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {'action_space': gym.spaces.Box(0.0, 1.0)}\n",
    "\n",
    "test_environment(UniformlyRestrictedEnvironment(\n",
    "    {'env': DummyEnvironment, 'env_config': env_config, 'governance_action_space': gym.spaces.Box(0.0, 1.0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Dict('action_range': Box(0.0, 360.0, (1,), float32), 'dt': Box(0.0, 5.0, (1,), float32), 'location': Box(0.0, 20.0, (2,), float32), 'map': Box(0.0, 20.0, (2,), float32), 'perspective': Box(0.0, 360.0, (1,), float32), 'step': Box(0.0, 40.0, (1,), float32), 'step_radius': Box(0.0, 5.0, (1,), float32), 'step_size': Box(0.0, 5.0, (1,), float32))\n",
      "Testing <class 'src.wrappers.rllib.UniformlyRestrictedEnvironment'>...\n",
      "Reset: {'gov': {'location': array([1., 1.], dtype=float32), 'perspective': array([90.], dtype=float32), 'map': array([15., 15.], dtype=float32), 'step': array([0.], dtype=float32), 'step_radius': array([0.4], dtype=float32), 'step_size': array([1.], dtype=float32), 'action_range': array([220.], dtype=float32), 'dt': array([1.], dtype=float32)}}\n",
      "L\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 98\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgovernance_reward_fn\u001B[39m(wrapper):\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28msum\u001B[39m(wrapper\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m---> 98\u001B[0m \u001B[43mtest_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mUniformlyRestrictedEnvironment\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mNavigationEnvironment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv_config\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43menv_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_action_space\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_observation_space\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgovernance_observation_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_observation_fn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgovernance_observation_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_reward_fn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgovernance_reward_fn\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mNavigationRestrictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgovernance_observation_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mIntervalUnion\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m110.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m110.0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mgovernance_config\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Institute for Enterprise Systems/Paper/hicss-2024/src/utils.py:34\u001B[0m, in \u001B[0;36mtest_environment\u001B[0;34m(env, restrictor, env_config, num_steps, num_episodes, individual)\u001B[0m\n\u001B[1;32m     29\u001B[0m     actions \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     30\u001B[0m         agent_id: {agent: env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample() \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m env\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magents\n\u001B[1;32m     31\u001B[0m              } \u001B[38;5;28;01mif\u001B[39;00m agent_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample() \u001B[38;5;28;01mfor\u001B[39;00m agent_id \u001B[38;5;129;01min\u001B[39;00m obs\n\u001B[1;32m     32\u001B[0m     }\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 34\u001B[0m     actions \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     35\u001B[0m         agent_id: restrictor\u001B[38;5;241m.\u001B[39mcompute_actions([obs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m restrictor \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m obs \u001B[38;5;28;01melse\u001B[39;00m env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m agent_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample() \u001B[38;5;28;01mfor\u001B[39;00m agent_id \u001B[38;5;129;01min\u001B[39;00m obs\n\u001B[1;32m     38\u001B[0m     }\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mactions\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     40\u001B[0m obs, rewards, done, truncated, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n",
      "File \u001B[0;32m~/Documents/Institute for Enterprise Systems/Paper/hicss-2024/src/utils.py:35\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     29\u001B[0m     actions \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     30\u001B[0m         agent_id: {agent: env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample() \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m env\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magents\n\u001B[1;32m     31\u001B[0m              } \u001B[38;5;28;01mif\u001B[39;00m agent_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample() \u001B[38;5;28;01mfor\u001B[39;00m agent_id \u001B[38;5;129;01min\u001B[39;00m obs\n\u001B[1;32m     32\u001B[0m     }\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m     actions \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m---> 35\u001B[0m         agent_id: \u001B[43mrestrictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgov\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m restrictor \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m obs \u001B[38;5;28;01melse\u001B[39;00m env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m agent_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgov\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m env\u001B[38;5;241m.\u001B[39maction_space[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msample() \u001B[38;5;28;01mfor\u001B[39;00m agent_id \u001B[38;5;129;01min\u001B[39;00m obs\n\u001B[1;32m     38\u001B[0m     }\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mactions\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     40\u001B[0m obs, rewards, done, truncated, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n",
      "File \u001B[0;32m~/Documents/Institute for Enterprise Systems/Paper/hicss-2024/examples/restrictors/navigation_restrictor.py:75\u001B[0m, in \u001B[0;36mNavigationRestrictor.compute_actions\u001B[0;34m(self, obs_batch, state_batches, prev_action_batch, prev_reward_batch, info_batch, episodes, **kwargs)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_actions\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     67\u001B[0m                     obs_batch,\n\u001B[1;32m     68\u001B[0m                     state_batches\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m                     episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m                     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m     obs_batch \u001B[38;5;241m=\u001B[39m \u001B[43mrestore_original_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumpy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m     actions \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(obs_batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/models/modelv2.py:411\u001B[0m, in \u001B[0;36mrestore_original_dimensions\u001B[0;34m(obs, obs_space, tensorlib)\u001B[0m\n\u001B[1;32m    409\u001B[0m     tensorlib \u001B[38;5;241m=\u001B[39m np\n\u001B[1;32m    410\u001B[0m original_space \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obs_space, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moriginal_space\u001B[39m\u001B[38;5;124m\"\u001B[39m, obs_space)\n\u001B[0;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_unpack_obs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moriginal_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensorlib\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorlib\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/models/modelv2.py:444\u001B[0m, in \u001B[0;36m_unpack_obs\u001B[0;34m(obs, space, tensorlib)\u001B[0m\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_cache) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m999\u001B[39m:\n\u001B[1;32m    443\u001B[0m         _cache[\u001B[38;5;28mid\u001B[39m(space)] \u001B[38;5;241m=\u001B[39m prep\n\u001B[0;32m--> 444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mobs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m obs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m!=\u001B[39m prep\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected flattened obs shape of [..., \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m], got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    447\u001B[0m             prep\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], obs\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    448\u001B[0m         )\n\u001B[1;32m    449\u001B[0m     )\n\u001B[1;32m    450\u001B[0m offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env_config = {\n",
    "    'STEPS_PER_EPISODE': 4,\n",
    "    'ACTION_RANGE': 220,\n",
    "    'DT': 1.0,\n",
    "    'REWARD': {\n",
    "        'TIMESTEP_PENALTY_COEFFICIENT': 0.05,\n",
    "        'REWARD_COEFFICIENT': 5.0,\n",
    "        'GOAL': 50,\n",
    "        'COLLISION': -20.0\n",
    "    },\n",
    "    'MAP': {\n",
    "        'HEIGHT': 15.0,\n",
    "        'WIDTH': 15.0,\n",
    "        'AGENT': {'x': 1.0, 'y': 1.0, 'angle': 90.0, 'step_size': 1.0, 'radius': 0.4},\n",
    "        'GOAL': {'x': 12.0, 'y': 12.0, 'radius': 0.5}\n",
    "    }\n",
    "}\n",
    "\n",
    "governance_config = {\n",
    "    'COUNT': 3,\n",
    "    'POSITION_COVARIANCE': [[8.0, 0.0],\n",
    "                            [0.0, 8.0]],\n",
    "    'MEAN_SIZE': 1.0,\n",
    "    'VARIANCE_SIZE': 0.25,\n",
    "    'RANGE_SIZE': 0.75,\n",
    "    'START_SEED': 50,\n",
    "    'SAFETY_ANGLE': 6\n",
    "}\n",
    "\n",
    "governance_observation_space = Dict({'location': Box(0.0, 20.0, shape=(2,)),\n",
    "                                                'perspective': Box(0.0, 360.0, shape=(1,)),\n",
    "                                                'map': Box(0.0, 20.0, shape=(2,)),\n",
    "                                                'step': Box(0.0, 40.0, shape=(1,)),\n",
    "                                                'step_radius': Box(0.0, 5.0, shape=(1,)),\n",
    "                                                'step_size': Box(0.0, 5.0, shape=(1,)),\n",
    "                                                'action_range': Box(0.0, 360.0, shape=(1,)),\n",
    "                                                'dt': Box(0.0, 5.0, shape=(1,))})\n",
    "\n",
    "model_config = {\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    print(agent_id)\n",
    "    return agent_id\n",
    "\n",
    "\n",
    "run_config = {\n",
    "    'env': UniformlyRestrictedEnvironment,\n",
    "    'env_config': env_config,\n",
    "    'multiagent': {\n",
    "        'policies':\n",
    "            {\n",
    "                'agent': PolicySpec(\n",
    "                    policy_class=MPSTD3Policy,\n",
    "                    observation_space=Dict({\n",
    "                        'observation': Dict({\n",
    "                            'location': Box(low=0.0, high=15.0, shape=(2,), dtype=np.float32),\n",
    "                            'perspective': Box(low=0.0, high=360.0, shape=(1,), dtype=np.float32),\n",
    "                            'target_angle': Box(low=0.0, high=360.0, shape=(1,), dtype=np.float32),\n",
    "                            'target_distance': Box(low=0.0, high=50.0, shape=(1,), dtype=np.float32),\n",
    "                            'current_step': Box(low=0.0, high=40.0, shape=(1,), dtype=np.float32)\n",
    "                        }),\n",
    "                        'allowed_actions': IntervalUnion(-110.0, 110.0)\n",
    "                    }),\n",
    "                    action_space=Box(low=-110.0, high=110.0, shape=(1,), dtype=np.float32),\n",
    "                    config=model_config\n",
    "                ),\n",
    "                'gov': PolicySpec(\n",
    "                    policy_class=NavigationRestrictor,\n",
    "                    observation_space=governance_observation_space,\n",
    "                    action_space=IntervalUnion(-110.0, 110.0),\n",
    "                    config=governance_config\n",
    "                )\n",
    "            },\n",
    "        'policy_mapping_fn': policy_mapping_fn,\n",
    "        'policies_to_train': ['agent']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def governance_observation_fn(wrapper):\n",
    "    return {'location': np.array([wrapper.env.agent.x, wrapper.env.agent.y], dtype=np.float32),\n",
    "            'perspective': np.array([wrapper.env.agent.perspective], dtype=np.float32),\n",
    "            'map': np.array([wrapper.env.HEIGHT, wrapper.env.WIDTH], dtype=np.float32),\n",
    "            'step': np.array([wrapper.env.current_step], dtype=np.float32),\n",
    "            'step_radius': np.array([wrapper.env.agent.radius], dtype=np.float32),\n",
    "            'step_size': np.array([wrapper.env.agent.step_size], dtype=np.float32),\n",
    "            'action_range': np.array([wrapper.env.ACTION_RANGE], dtype=np.float32),\n",
    "            'dt': np.array([wrapper.env.DT], dtype=np.float32)}\n",
    "\n",
    "\n",
    "def governance_reward_fn(wrapper):\n",
    "    return -sum(wrapper.rewards.values())\n",
    "\n",
    "\n",
    "test_environment(UniformlyRestrictedEnvironment({'env': NavigationEnvironment, 'env_config': env_config,\n",
    "                                                 'governance_action_space': Box(low=0.0, high=1.0, shape=(2,)),\n",
    "                                                 'governance_observation_space': governance_observation_space,\n",
    "                                                 'governance_observation_fn': governance_observation_fn,\n",
    "                                                 'governance_reward_fn': governance_reward_fn}),\n",
    "                 NavigationRestrictor(governance_observation_space,\n",
    "                                      IntervalUnion(-110.0, 110.0),\n",
    "                                      governance_config))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 17:41:07,371\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:61: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/numpy/dtype.py:82: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m   bool = np.bool  # pylint: disable=redefined-builtin\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/numpy/dtype.py:112: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m   string = getattr(np, 'str', getattr(np, 'string', None))\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m WARNING:tensorflow:From /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m experimental_compile is deprecated, use jit_compile instead\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m WARNING:tensorflow:From /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:61: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/tensorflow_probability/python/mcmc/sample_halton_sequence.py:373: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "\u001B[2m\u001B[36m(pid=78480)\u001B[0m   sieve = np.ones(n // 3 + (n % 6 == 2), dtype=np.bool)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m 2023-05-22 17:41:22,170\tWARNING algorithm_config.py:635 -- Cannot create DDPGConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m 2023-05-22 17:41:22,855\tINFO algorithm.py:527 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m 2023-05-22 17:41:22,866\tWARNING env.py:285 -- Your MultiAgentEnv <UniformlyRestrictedEnvironment instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (8,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (47,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (47,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (47,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (47,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-110.0, 110.0, (1,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (1,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-110.0, 110.0, (1,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (1,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (47,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (47,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (47,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (47,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-110.0, 110.0, (1,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (1,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-110.0, 110.0, (1,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (1,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (43,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (43,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m hi\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-1.0, 1.0, (10,), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m process {0: {'gov': {'location': array([1., 1.], dtype=float32), 'perspective': array([90.], dtype=float32), 'map': array([15., 15.], dtype=float32), 'step': array([0.], dtype=float32), 'step_radius': array([0.4], dtype=float32), 'step_size': array([1.], dtype=float32), 'action_range': array([220.], dtype=float32), 'dt': array([1.], dtype=float32)}}}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {0: {'gov': {'location': array([1., 1.], dtype=float32), 'perspective': array([90.], dtype=float32), 'map': array([15., 15.], dtype=float32), 'step': array([0.], dtype=float32), 'step_radius': array([0.4], dtype=float32), 'step_size': array([1.], dtype=float32), 'action_range': array([220.], dtype=float32), 'dt': array([1.], dtype=float32)}}}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m gov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m 2023-05-22 17:41:23,036\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "2023-05-22 17:41:23,144\tERROR trial_runner.py:1450 -- Trial DDPG_UniformlyRestrictedEnvironment_12f10_00000: Error happened when processing _ExecutorEventType.TRAINING_RESULT.\n",
      "ray.exceptions.RayTaskError(AssertionError): \u001B[36mray::DDPG.train()\u001B[39m (pid=78480, ip=127.0.0.1, repr=DDPG)\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 384, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 381, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 792, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2811, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/algorithms/simple_q/simple_q.py\", line 319, in training_step\n",
      "    new_sample_batches = synchronous_parallel_sample(\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 82, in synchronous_parallel_sample\n",
      "    sample_batches = [worker_set.local_worker().sample()]\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n",
      "    outputs = self.step()\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 350, in step\n",
      "    active_envs, to_eval, outputs = self._process_observations(\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 679, in _process_observations\n",
      "    sample_batch = self._try_build_truncated_episode_multi_agent_batch(\n",
      "  File \"/Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 967, in _try_build_truncated_episode_multi_agent_batch\n",
      "    assert built_steps + ongoing_steps == self._rollout_fragment_length, (\n",
      "AssertionError: built_steps (2) + ongoing_steps (0) != rollout_fragment_length (1).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div class=\"trialProgress\">\n  <h3>Trial Progress</h3>\n  <table>\n<thead>\n<tr><th>Trial name                                     </th><th>date               </th><th>hostname                      </th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id  </th></tr>\n</thead>\n<tbody>\n<tr><td>DDPG_UniformlyRestrictedEnvironment_12f10_00000</td><td>2023-05-22_17-41-23</td><td>vpn-135-057.rz.uni-mannheim.de</td><td>127.0.0.1</td><td style=\"text-align: right;\">78480</td><td style=\"text-align: right;\"> 1684770083</td><td>default   </td></tr>\n</tbody>\n</table>\n</div>\n<style>\n.trialProgress {\n  display: flex;\n  flex-direction: column;\n  color: var(--jp-ui-font-color1);\n}\n.trialProgress h3 {\n  font-weight: bold;\n}\n.trialProgress td {\n  white-space: nowrap;\n}\n</style>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m 2023-05-22 17:41:23,115\tWARNING env_runner_v2.py:991 -- Your environment seems to be stepping w/o ever emitting agent observations (agents are never requested to act)!\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m /Users/tim/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/rllib/evaluation/collectors/agent_collector.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m   arr = np.array(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'obs': ViewRequirement(data_col=None, space=Box(-1.0, 1.0, (10,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'new_obs': ViewRequirement(data_col='obs', space=Box(-1.0, 1.0, (10,), float32), shift=1, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([1])), 'actions': ViewRequirement(data_col=None, space=<IntervalUnion>, shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'prev_actions': ViewRequirement(data_col='actions', space=<IntervalUnion>, shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'rewards': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'prev_rewards': ViewRequirement(data_col='rewards', space=Box(-inf, inf, (), float32), shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'terminateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'truncateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'infos': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'eps_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'unroll_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'agent_index': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 't': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'state_in_0': ViewRequirement(data_col='state_out_0', space=[-180.0, 180.0], shift=-1, index=None, batch_repeat_value=20, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'state_out_0': ViewRequirement(data_col=None, space=[-180.0, 180.0], shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0]))}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m <IntervalUnion>\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (2,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'obs': ViewRequirement(data_col=None, space=Box(-1.0, 1.0, (10,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'new_obs': ViewRequirement(data_col='obs', space=Box(-1.0, 1.0, (10,), float32), shift=1, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([1])), 'actions': ViewRequirement(data_col=None, space=<IntervalUnion>, shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'prev_actions': ViewRequirement(data_col='actions', space=<IntervalUnion>, shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'rewards': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'prev_rewards': ViewRequirement(data_col='rewards', space=Box(-inf, inf, (), float32), shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'terminateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'truncateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'infos': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'eps_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'unroll_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'agent_index': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 't': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'state_in_0': ViewRequirement(data_col='state_out_0', space=[-180.0, 180.0], shift=-1, index=None, batch_repeat_value=20, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'state_out_0': ViewRequirement(data_col=None, space=[-180.0, 180.0], shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0]))}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m <IntervalUnion>\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (2,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m L\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m hii\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m actions\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'gov': array([[-110.  ,   30.92]], dtype=float32)}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m process {0: {'agent': {'observation': {'location': array([1., 1.], dtype=float32), 'perspective': array([90.], dtype=float32), 'target_angle': array([45.], dtype=float32), 'target_distance': array([0.], dtype=float32), 'current_step': array([0.], dtype=float32)}, 'allowed_actions': array([[-110.  ,   30.92]], dtype=float32)}}}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {0: {'agent': {'observation': {'location': array([1., 1.], dtype=float32), 'perspective': array([90.], dtype=float32), 'target_angle': array([45.], dtype=float32), 'target_distance': array([0.], dtype=float32), 'current_step': array([0.], dtype=float32)}, 'allowed_actions': array([[-110.  ,   30.92]], dtype=float32)}}}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m agent\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'obs': ViewRequirement(data_col=None, space=Box(-1.0, 1.0, (47,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'new_obs': ViewRequirement(data_col='obs', space=Box(-1.0, 1.0, (47,), float32), shift=1, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([1])), 'actions': ViewRequirement(data_col=None, space=Box(-110.0, 110.0, (1,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'rewards': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'terminateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'truncateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'infos': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'eps_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'unroll_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'agent_index': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 't': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'weights': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0]))}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'obs': ViewRequirement(data_col=None, space=Box(-1.0, 1.0, (47,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'new_obs': ViewRequirement(data_col='obs', space=Box(-1.0, 1.0, (47,), float32), shift=1, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([1])), 'actions': ViewRequirement(data_col=None, space=Box(-110.0, 110.0, (1,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'rewards': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'terminateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'truncateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'infos': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'eps_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'unroll_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'agent_index': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 't': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'weights': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0]))}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m actions\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'agent': array([110.], dtype=float32)}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m process {0: {'gov': {'location': array([0.06031, 0.65798], dtype=float32), 'perspective': array([200.], dtype=float32), 'map': array([15., 15.], dtype=float32), 'step': array([1.], dtype=float32), 'step_radius': array([0.4], dtype=float32), 'step_size': array([1.], dtype=float32), 'action_range': array([220.], dtype=float32), 'dt': array([1.], dtype=float32)}}}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {0: {'gov': {'location': array([0.06031, 0.65798], dtype=float32), 'perspective': array([200.], dtype=float32), 'map': array([15., 15.], dtype=float32), 'step': array([1.], dtype=float32), 'step_radius': array([0.4], dtype=float32), 'step_size': array([1.], dtype=float32), 'action_range': array([220.], dtype=float32), 'dt': array([1.], dtype=float32)}}}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m gov\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'obs': ViewRequirement(data_col=None, space=Box(-1.0, 1.0, (10,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'new_obs': ViewRequirement(data_col='obs', space=Box(-1.0, 1.0, (10,), float32), shift=1, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([1])), 'actions': ViewRequirement(data_col=None, space=<IntervalUnion>, shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'prev_actions': ViewRequirement(data_col='actions', space=<IntervalUnion>, shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'rewards': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'prev_rewards': ViewRequirement(data_col='rewards', space=Box(-inf, inf, (), float32), shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'terminateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'truncateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'infos': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'eps_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'unroll_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'agent_index': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 't': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'state_in_0': ViewRequirement(data_col='state_out_0', space=[-180.0, 180.0], shift=-1, index=None, batch_repeat_value=20, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'state_out_0': ViewRequirement(data_col=None, space=[-180.0, 180.0], shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0]))}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m <IntervalUnion>\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (2,)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m Box(-inf, inf, (), float32)\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m ()\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m {'obs': ViewRequirement(data_col=None, space=Box(-1.0, 1.0, (10,), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'new_obs': ViewRequirement(data_col='obs', space=Box(-1.0, 1.0, (10,), float32), shift=1, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([1])), 'actions': ViewRequirement(data_col=None, space=<IntervalUnion>, shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'prev_actions': ViewRequirement(data_col='actions', space=<IntervalUnion>, shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'rewards': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'prev_rewards': ViewRequirement(data_col='rewards', space=Box(-inf, inf, (), float32), shift=-1, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'terminateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'truncateds': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'infos': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=False, used_for_training=True, shift_arr=array([0])), 'eps_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'unroll_id': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'agent_index': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 't': ViewRequirement(data_col=None, space=Box(-inf, inf, (), float32), shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0])), 'state_in_0': ViewRequirement(data_col='state_out_0', space=[-180.0, 180.0], shift=-1, index=None, batch_repeat_value=20, used_for_compute_actions=True, used_for_training=True, shift_arr=array([-1])), 'state_out_0': ViewRequirement(data_col=None, space=[-180.0, 180.0], shift=0, index=None, batch_repeat_value=1, used_for_compute_actions=True, used_for_training=True, shift_arr=array([0]))}\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m <IntervalUnion>\n",
      "\u001B[2m\u001B[36m(DDPG pid=78480)\u001B[0m (2,)\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [DDPG_UniformlyRestrictedEnvironment_12f10_00000])",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTuneError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mray\u001B[39;00m\n\u001B[1;32m      4\u001B[0m ray\u001B[38;5;241m.\u001B[39minit()\n\u001B[0;32m----> 5\u001B[0m analysis \u001B[38;5;241m=\u001B[39m \u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDDPG\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mUniformlyRestrictedEnvironment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv_config\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mNavigationEnvironment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv_config\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43menv_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_action_space\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mRepeated\u001B[49m\u001B[43m(\u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m180.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m180.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_observation_space\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgovernance_observation_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_observation_fn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgovernance_observation_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgovernance_reward_fn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgovernance_reward_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m                            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmultiagent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpolicies\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m                                        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43magent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mPolicySpec\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mpolicy_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMPSTD3Policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mobservation_space\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m                                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mobservation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mDict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlocation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mperspective\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m360.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m                                                                       \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget_angle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m360.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m                                                                        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget_distance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m                                                                           \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcurrent_step\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m40.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m                                                                        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m                                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mallowed_actions\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mRepeated\u001B[49m\u001B[43m(\u001B[49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m180.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m180.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m                                                                                \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m110.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m110.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_config\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgov\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mPolicySpec\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mpolicy_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNavigationRestrictor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mobservation_space\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgovernance_observation_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mIntervalUnion\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m180.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m180.0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgovernance_config\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpolicy_mapping_fn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_mapping_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpolicies_to_train\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43magent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m                            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m                            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mframework\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtorch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m     49\u001B[0m \u001B[43m                        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnum_env_steps_sampled\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5000\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024/lib/python3.10/site-packages/ray/tune/tune.py:939\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, _experiment_checkpoint_dir, _remote, _remote_string_queue, _tuner_api)\u001B[0m\n\u001B[1;32m    937\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m incomplete_trials:\n\u001B[1;32m    938\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m raise_on_failed_trial \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m experiment_interrupted_event\u001B[38;5;241m.\u001B[39mis_set():\n\u001B[0;32m--> 939\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m TuneError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrials did not complete\u001B[39m\u001B[38;5;124m\"\u001B[39m, incomplete_trials)\n\u001B[1;32m    940\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    941\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrials did not complete: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, incomplete_trials)\n",
      "\u001B[0;31mTuneError\u001B[0m: ('Trials did not complete', [DDPG_UniformlyRestrictedEnvironment_12f10_00000])"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.spaces.repeated import Repeated\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "analysis = ray.tune.run('DDPG',\n",
    "                        config={\n",
    "                            'env': UniformlyRestrictedEnvironment,\n",
    "                            'env_config': {\n",
    "                                'env': NavigationEnvironment, 'env_config': env_config,\n",
    "                                'governance_action_space': Repeated(Box(low=-180.0, high=180.0, shape=(2,)), max_len=20),\n",
    "                                'governance_observation_space': governance_observation_space,\n",
    "                                'governance_observation_fn': governance_observation_fn,\n",
    "                                'governance_reward_fn': governance_reward_fn,\n",
    "                            },\n",
    "                            'multiagent': {\n",
    "                                'policies':\n",
    "                                    {\n",
    "                                        'agent': PolicySpec(\n",
    "                                            policy_class=MPSTD3Policy,\n",
    "                                            observation_space=Dict({\n",
    "                                                'observation': Dict({\n",
    "                                                    'location': Box(low=0.0, high=15.0, shape=(2,), dtype=np.float32),\n",
    "                                                    'perspective': Box(low=0.0, high=360.0, shape=(1,),\n",
    "                                                                       dtype=np.float32),\n",
    "                                                    'target_angle': Box(low=0.0, high=360.0, shape=(1,),\n",
    "                                                                        dtype=np.float32),\n",
    "                                                    'target_distance': Box(low=0.0, high=50.0, shape=(1,),\n",
    "                                                                           dtype=np.float32),\n",
    "                                                    'current_step': Box(low=0.0, high=40.0, shape=(1,),\n",
    "                                                                        dtype=np.float32)\n",
    "                                                }),\n",
    "                                                'allowed_actions': Repeated(Box(low=-180.0, high=180.0, shape=(2,),\n",
    "                                                                                dtype=np.float32), max_len=20)\n",
    "                                            }),\n",
    "                                            action_space=Box(low=-110.0, high=110.0, shape=(1,), dtype=np.float32),\n",
    "                                            config=model_config\n",
    "                                        ),\n",
    "                                        'gov': PolicySpec(\n",
    "                                            policy_class=NavigationRestrictor,\n",
    "                                            observation_space=governance_observation_space,\n",
    "                                            action_space=IntervalUnion(-180.0, 180.0),\n",
    "                                            config=governance_config\n",
    "                                        )\n",
    "                                    },\n",
    "                                'policy_mapping_fn': policy_mapping_fn,\n",
    "                                'policies_to_train': ['agent']\n",
    "                            },\n",
    "                            'framework': 'torch'\n",
    "                        },\n",
    "                        stop={'num_env_steps_sampled': 5000})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
