{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from src.wrapper import RestrictionWrapper\n",
    "from examples.agents.td3 import TD3\n",
    "from examples.envs.navigation import NavigationEnvironment\n",
    "from examples.restrictors.navigation_restrictor import NavigationRestrictor\n",
    "from examples.utils import ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Tested with seeds 45, 46, 47, 48, 49\n",
    "seed = 49\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'HEIGHT': 15.0,\n",
    "    'WIDTH': 15.0,\n",
    "    'STEPS_PER_EPISODE': 40,\n",
    "    'ACTION_RANGE': 220,\n",
    "    'DT': 1.0,\n",
    "    'TIMESTEP_PENALTY_COEFFICIENT': 0.05,\n",
    "    'REWARD_COLLISION': -1.0,\n",
    "    'REWARD_GOAL': 5.0,\n",
    "    'REWARD_COEFFICIENT': 10.0,\n",
    "    'AGENT_RADIUS': 0.5,\n",
    "    'AGENT_PERSPECTIVE': 90,\n",
    "    'AGENT_STEP_SIZE': 1.0,\n",
    "    'AGENT_X': 1.5,\n",
    "    'AGENT_Y': 1.5,\n",
    "    'GOAL_RADIUS': 1.0,\n",
    "    'GOAL_X': 12.0,\n",
    "    'GOAL_y': 12.0\n",
    "}\n",
    "\n",
    "environment = NavigationEnvironment(env_config)\n",
    "restrictor = NavigationRestrictor(obstacle_count=4,\n",
    "                                  obstacle_position_covariance=[[5.0, 0.0], [0.0, 5.0]],\n",
    "                                  obstacle_mean_size=1.0,\n",
    "                                  obstacle_variance_size=0.2,\n",
    "                                  obstacle_size_range=0.5,\n",
    "                                  start_seed=50,\n",
    "                                  safety_angle=8,\n",
    "                                  min_angle=-110.0,\n",
    "                                  max_angle=110.0)\n",
    "restricted_environment = RestrictionWrapper(environment, restrictor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "td3_config = {\n",
    "    'state_dim': 6,\n",
    "    'action_dim': 1,\n",
    "    'max_action': 110.0,\n",
    "    'discount': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'policy_noise': 0.2,\n",
    "    'noise_clip:': 0.5,\n",
    "    'policy_freq': 2,\n",
    "    'exploration_noise': 0.2,\n",
    "    'batch_size': 256,\n",
    "    'train_after_timesteps': 2000,\n",
    "    'learning_rate_actor': 1e-5,\n",
    "    'learning_rate_critic': 1e-5\n",
    "}\n",
    "\n",
    "total_timesteps = 50000\n",
    "td3 = TD3(**td3_config)\n",
    "replay_buffer = ReplayBuffer(state_dim=td3_config['state_dim'], action_dim=td3_config['action_dim'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from src.restrictors import Restrictor\n",
    "\n",
    "\n",
    "def evaluate(eval_policy: TD3, eval_restrictor: Restrictor):\n",
    "    eval_env = RestrictionWrapper(NavigationEnvironment(env_config),\n",
    "                                  NavigationRestrictor(obstacle_count=4,\n",
    "                                                       obstacle_position_covariance=[[5.0, 0.0], [0.0, 5.0]],\n",
    "                                                       obstacle_mean_size=1.0,\n",
    "                                                       obstacle_variance_size=0.2,\n",
    "                                                       obstacle_size_range=0.5,\n",
    "                                                       start_seed=1,\n",
    "                                                       safety_angle=8,\n",
    "                                                       min_angle=-110.0,\n",
    "                                                       max_angle=110.0))\n",
    "    eval_reward = 0.0\n",
    "\n",
    "    eval_env.reset()\n",
    "    for eval_agent in eval_env.agent_iter():\n",
    "        obs, rew, term, trunc, inf = eval_env.last()\n",
    "        if eval_agent == 'agent_0':\n",
    "            eval_reward += rew\n",
    "            eval_action = np.array([\n",
    "                obs['restriction'].nearest_element(eval_policy.select_action(obs['observation'])[0])\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            eval_action = eval_restrictor.act(obs)\n",
    "\n",
    "        if term or trunc:\n",
    "            eval_action = None\n",
    "\n",
    "        eval_env.step(eval_action)\n",
    "\n",
    "    return eval_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.12133494725474\n",
      "Finished episode 1 with reward 17.102499987244716 in 24 steps - goal reached: True\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 2 with reward -7.000763217582208 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 3 with reward 9.183637545550695 in 41 steps - goal reached: False\n",
      "Finished episode 4 with reward -19.8016873915515 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 5 with reward 35.22606793419044 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 6 with reward 30.676594723818763 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "Finished episode 7 with reward 22.63403324342613 in 22 steps - goal reached: True\n",
      "Finished episode 8 with reward 22.235666572965407 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 9 with reward 25.704908850925023 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 10 with reward 75.93506544284361 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 11 with reward 19.696671780288803 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 12 with reward -3.7963357239488387 in 41 steps - goal reached: False\n",
      "Finished episode 13 with reward 36.40705934060024 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 14 with reward -46.519063286515916 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 15 with reward -22.68953166875103 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 16 with reward 47.881449904797556 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 17 with reward 32.418007958720594 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 18 with reward -18.646689953260672 in 41 steps - goal reached: False\n",
      "Finished episode 19 with reward -10.017963854152 in 3 steps - goal reached: True\n",
      "Finished episode 20 with reward 121.96081155520109 in 27 steps - goal reached: True\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 21 with reward 65.92406349650051 in 41 steps - goal reached: False\n",
      "Finished episode 22 with reward -20.30836277305034 in 23 steps - goal reached: True\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 23 with reward 15.554474541733969 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 24 with reward 60.29376827152727 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 25 with reward 28.54661766332177 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 26 with reward 12.864620119215854 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "Finished episode 27 with reward 29.31822274896303 in 41 steps - goal reached: False\n",
      "Finished episode 28 with reward 10.785829220566727 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 29 with reward 63.00268734008711 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 30 with reward 28.254165105407637 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 31 with reward -19.53460053521916 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 32 with reward 5.46047293351841 in 41 steps - goal reached: False\n",
      "Finished episode 33 with reward 81.57411646378213 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 34 with reward 47.60748580316113 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 35 with reward -1.831022342827687 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 36 with reward -36.12088698796381 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 37 with reward -10.222908331910526 in 32 steps - goal reached: True\n",
      "Finished episode 38 with reward -20.473193773030495 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 39 with reward 45.287172164171366 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 40 with reward -31.660538306031818 in 32 steps - goal reached: True\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 41 with reward -16.741291557919762 in 41 steps - goal reached: False\n",
      "Finished episode 42 with reward 31.81349814029307 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 43 with reward -9.604976780742579 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 44 with reward 33.639773352946236 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 45 with reward 52.70632711689984 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 46 with reward 35.340657594562494 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 47 with reward -0.6303187930687351 in 41 steps - goal reached: False\n",
      "Finished episode 48 with reward -4.1472144428667015 in 8 steps - goal reached: True\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 49 with reward 2.110007950531601 in 41 steps - goal reached: False\n",
      "Finished episode 50 with reward -31.93801527496038 in 35 steps - goal reached: True\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 51 with reward 17.905744340491843 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 52 with reward -21.006045061504537 in 41 steps - goal reached: False\n",
      "-10.12133494725474\n",
      "-10.12133494725474\n",
      "Finished episode 53 with reward 77.2353463207351 in 41 steps - goal reached: False\n",
      "Finished episode 54 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "-10.12158948958428\n",
      "-10.12158948958428\n",
      "Finished episode 55 with reward 78.81511505887008 in 41 steps - goal reached: False\n",
      "Finished episode 56 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 57 with reward -9.94875526070216 in 3 steps - goal reached: True\n",
      "Finished episode 58 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 59 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 60 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "-10.12159365074526\n",
      "-10.12159365074526\n",
      "Finished episode 61 with reward 80.76240903949517 in 41 steps - goal reached: False\n",
      "Finished episode 62 with reward -10.09605938117398 in 3 steps - goal reached: True\n",
      "Finished episode 63 with reward 107.05411840846494 in 34 steps - goal reached: True\n",
      "-10.12158532841383\n",
      "Finished episode 64 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "-10.12158532841383\n",
      "Finished episode 65 with reward -9.75053753228208 in 3 steps - goal reached: True\n",
      "Finished episode 66 with reward 35.059395753702496 in 41 steps - goal reached: False\n",
      "-10.12144800448305\n",
      "-10.12144800448305\n",
      "Finished episode 67 with reward 79.95391895181018 in 41 steps - goal reached: False\n",
      "-4.301397842221567\n",
      "-4.301397842221567\n",
      "Finished episode 68 with reward 9.634660983782656 in 30 steps - goal reached: True\n",
      "119.90126611360694\n",
      "119.90126611360694\n",
      "Finished episode 69 with reward 62.26105966361712 in 41 steps - goal reached: False\n",
      "Finished episode 70 with reward 66.18974008766904 in 41 steps - goal reached: False\n",
      "119.90120098591993\n",
      "119.90120098591993\n",
      "Finished episode 71 with reward 65.7173549053152 in 41 steps - goal reached: False\n",
      "3.9744249854606206\n",
      "121.07888867641495\n",
      "Finished episode 72 with reward 43.28464949992108 in 41 steps - goal reached: False\n",
      "119.90120098591993\n",
      "62.62585758434402\n",
      "Finished episode 73 with reward 56.88024197110697 in 41 steps - goal reached: False\n",
      "Finished episode 74 with reward 23.742693510573027 in 20 steps - goal reached: True\n",
      "118.93828686981172\n",
      "119.90120098591993\n",
      "Finished episode 75 with reward 56.69640253269126 in 41 steps - goal reached: False\n",
      "119.90120098591993\n",
      "119.90120098591993\n",
      "Finished episode 76 with reward 80.82771071065078 in 41 steps - goal reached: False\n",
      "67.06006253359064\n",
      "119.90120098591993\n",
      "Finished episode 77 with reward 25.210164076923938 in 41 steps - goal reached: False\n",
      "119.90120098591993\n",
      "-5.212126447955318\n",
      "Finished episode 78 with reward 13.81883126591188 in 41 steps - goal reached: False\n",
      "Finished episode 79 with reward 47.928192335905045 in 41 steps - goal reached: False\n",
      "119.90120098591993\n",
      "119.90120098591993\n",
      "Finished episode 80 with reward -41.49010957398601 in 41 steps - goal reached: False\n",
      "119.90120802907647\n",
      "119.90845258106098\n",
      "Finished episode 81 with reward 118.97565080312214 in 30 steps - goal reached: True\n",
      "119.89928647769561\n",
      "119.89928647769561\n",
      "Finished episode 82 with reward 77.28741763388723 in 41 steps - goal reached: False\n",
      "Finished episode 83 with reward 49.947076971780156 in 41 steps - goal reached: False\n",
      "119.8599154188481\n",
      "119.8599154188481\n",
      "Finished episode 84 with reward 29.60132699472674 in 41 steps - goal reached: False\n",
      "111.59856652842042\n",
      "21.792358818116565\n",
      "Finished episode 85 with reward 42.8811762539902 in 41 steps - goal reached: False\n",
      "-7.088407275149573\n",
      "-47.33833404823344\n",
      "Finished episode 86 with reward 77.8721343580186 in 33 steps - goal reached: True\n",
      "126.01349125281413\n",
      "125.39668670292612\n",
      "Finished episode 87 with reward 10.97270765272317 in 41 steps - goal reached: False\n",
      "Finished episode 88 with reward 19.906395852963186 in 41 steps - goal reached: False\n",
      "-20.175777660294425\n",
      "-31.696262320472172\n",
      "Finished episode 89 with reward -4.292194879808354 in 41 steps - goal reached: False\n",
      "-7.7795200590191005\n",
      "-50.543585462605165\n",
      "Finished episode 90 with reward 110.7131425940076 in 30 steps - goal reached: True\n",
      "128.46443180784865\n",
      "-3.979079560667893\n",
      "Finished episode 91 with reward 26.218886226163214 in 41 steps - goal reached: False\n",
      "Finished episode 92 with reward 128.68988686906766 in 21 steps - goal reached: True\n",
      "-13.191303979075919\n",
      "118.01404131220208\n",
      "Finished episode 93 with reward -15.637778166374147 in 41 steps - goal reached: False\n",
      "Finished episode 94 with reward 127.84390049823898 in 20 steps - goal reached: True\n",
      "-37.5360077345783\n",
      "-25.7335602855551\n",
      "Finished episode 95 with reward -3.781631362559132 in 41 steps - goal reached: False\n",
      "Finished episode 96 with reward 128.22692097134123 in 17 steps - goal reached: True\n",
      "129.94810143399604\n",
      "129.94810143399604\n",
      "Finished episode 97 with reward 103.3328404990024 in 35 steps - goal reached: True\n",
      "123.95013131914041\n",
      "123.95013131914041\n",
      "Finished episode 98 with reward 119.38667262850366 in 23 steps - goal reached: True\n",
      "Finished episode 99 with reward 3.779761883185632 in 41 steps - goal reached: False\n",
      "127.43926895600633\n",
      "127.43926895600633\n",
      "Finished episode 100 with reward 131.89685317555168 in 17 steps - goal reached: True\n",
      "Finished episode 101 with reward 122.01137067113399 in 22 steps - goal reached: True\n",
      "130.8938369729247\n",
      "130.8938369729247\n",
      "Finished episode 102 with reward 130.22335124479008 in 17 steps - goal reached: True\n",
      "Finished episode 103 with reward 127.80833291599595 in 17 steps - goal reached: True\n",
      "Finished episode 104 with reward 132.83612587722212 in 16 steps - goal reached: True\n",
      "111.89526325594409\n",
      "128.14174965189116\n",
      "Finished episode 105 with reward 129.01074633028378 in 20 steps - goal reached: True\n",
      "Finished episode 106 with reward 112.65326788876217 in 28 steps - goal reached: True\n",
      "-4.188628148206332\n",
      "-27.253640342538453\n",
      "Finished episode 107 with reward 126.99236693037041 in 20 steps - goal reached: True\n",
      "Finished episode 108 with reward 118.44464869671529 in 23 steps - goal reached: True\n",
      "Finished episode 109 with reward 122.12100655673068 in 20 steps - goal reached: True\n",
      "132.76746144654442\n",
      "132.87022634540187\n",
      "Finished episode 110 with reward 121.48343993455798 in 21 steps - goal reached: True\n",
      "Finished episode 111 with reward 124.40162471478112 in 20 steps - goal reached: True\n",
      "126.88271359055996\n",
      "126.88271359055996\n",
      "Finished episode 112 with reward 132.7902369246705 in 17 steps - goal reached: True\n",
      "Finished episode 113 with reward 124.52227379051351 in 17 steps - goal reached: True\n",
      "Finished episode 114 with reward 126.12914876361626 in 20 steps - goal reached: True\n",
      "129.45515320119497\n",
      "129.45515320119497\n",
      "Finished episode 115 with reward 122.40465939134108 in 22 steps - goal reached: True\n",
      "Finished episode 116 with reward 129.01986891684933 in 17 steps - goal reached: True\n",
      "Finished episode 117 with reward 131.94484799500742 in 16 steps - goal reached: True\n",
      "130.2345071156343\n",
      "130.2345071156343\n",
      "Finished episode 118 with reward 124.02587838784925 in 17 steps - goal reached: True\n",
      "Finished episode 119 with reward 130.0279850064075 in 17 steps - goal reached: True\n",
      "Finished episode 120 with reward 125.629018701186 in 16 steps - goal reached: True\n",
      "127.60081815623207\n",
      "127.60081815623207\n",
      "Finished episode 121 with reward 125.92298253925152 in 16 steps - goal reached: True\n",
      "Finished episode 122 with reward 118.43592944807514 in 23 steps - goal reached: True\n",
      "130.313501777075\n",
      "127.78592501590434\n",
      "Finished episode 123 with reward 123.70646185672206 in 21 steps - goal reached: True\n",
      "Finished episode 124 with reward 118.22985351257626 in 23 steps - goal reached: True\n",
      "128.63208236922543\n",
      "Finished episode 125 with reward 125.79348240990183 in 21 steps - goal reached: True\n",
      "128.63208236922543\n",
      "Finished episode 126 with reward 129.22749056852808 in 16 steps - goal reached: True\n",
      "Finished episode 127 with reward 85.19503528286684 in 27 steps - goal reached: True\n",
      "132.74279103294782\n",
      "132.74279103294782\n",
      "Finished episode 128 with reward 60.5188068038118 in 41 steps - goal reached: False\n",
      "124.54874986969217\n",
      "Finished episode 129 with reward 129.828795086441 in 16 steps - goal reached: True\n",
      "124.54874986969217\n",
      "Finished episode 130 with reward 126.3016804310195 in 21 steps - goal reached: True\n",
      "Finished episode 131 with reward 131.41429151932024 in 17 steps - goal reached: True\n",
      "131.80774796068602\n",
      "-16.330657754282978\n",
      "Finished episode 132 with reward 76.85575731771918 in 41 steps - goal reached: False\n",
      "Finished episode 133 with reward 131.02179401575398 in 16 steps - goal reached: True\n",
      "129.3946497249157\n",
      "129.3946497249157\n",
      "Finished episode 134 with reward 130.5696371985519 in 17 steps - goal reached: True\n",
      "126.1902313213039\n",
      "132.43937666750628\n",
      "Finished episode 135 with reward 30.291638441771905 in 41 steps - goal reached: False\n",
      "Finished episode 136 with reward -4.804983034202712 in 41 steps - goal reached: False\n",
      "129.67939873097367\n",
      "129.67939873097367\n",
      "Finished episode 137 with reward 124.6719441483909 in 23 steps - goal reached: True\n",
      "130.01121246511042\n",
      "130.01121246511042\n",
      "Finished episode 138 with reward -39.98529774677106 in 41 steps - goal reached: False\n",
      "Finished episode 139 with reward 126.72787171138401 in 17 steps - goal reached: True\n",
      "Finished episode 140 with reward 122.27383655109604 in 21 steps - goal reached: True\n",
      "10.83875068146212\n",
      "-40.25166060557734\n",
      "Finished episode 141 with reward 126.96079232808542 in 21 steps - goal reached: True\n",
      "Finished episode 142 with reward 123.90752729054105 in 20 steps - goal reached: True\n",
      "129.94052742907664\n",
      "129.94052742907664\n",
      "Finished episode 143 with reward 128.99212764922112 in 17 steps - goal reached: True\n",
      "Finished episode 144 with reward 121.20169385236844 in 28 steps - goal reached: True\n",
      "129.7994660891485\n",
      "125.28136642442286\n",
      "Finished episode 145 with reward 121.71170537911115 in 24 steps - goal reached: True\n",
      "Finished episode 146 with reward 133.08203570307936 in 17 steps - goal reached: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 57\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m termination \u001B[38;5;129;01mor\u001B[39;00m truncation:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;66;03m# print(f'{action} for {next_observation[\"restriction\"]}, {restricted_environment.env.agent.x, restricted_environment.env.agent.y}')\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m \u001B[43mrestricted_environment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training_timesteps \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m50\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mprint\u001B[39m(evaluate(td3, restrictor))\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/src/wrapper.py:171\u001B[0m, in \u001B[0;36mRestrictionWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_selection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magent_selection\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# If the action was taken by an agent, execute it in the original\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# environment\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;66;03m# Update properties\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magents \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magents \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m    175\u001B[0m         \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_restrictor_mapping[agent] \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magents)\n\u001B[1;32m    176\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/examples/envs/navigation.py:214\u001B[0m, in \u001B[0;36mNavigationEnvironment.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mstep(step_direction, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDT)\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mlast_action \u001B[38;5;241m=\u001B[39m action\n\u001B[0;32m--> 214\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mcollided \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect_collision\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mset_distance_target(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistance_to_target())\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magent_0\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_reward()}\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/examples/envs/navigation.py:181\u001B[0m, in \u001B[0;36mNavigationEnvironment.detect_collision\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" Checks if the agent collided with the border\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \n\u001B[1;32m    177\u001B[0m \u001B[38;5;124;03mReturns:\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    violation (bool)\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;66;03m# Check if agent is on the map and not collided with the boundaries\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap\u001B[38;5;241m.\u001B[39mcontains(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgeometric_representation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mradius \u001B[38;5;241m-\u001B[39m Decimal(\n\u001B[1;32m    182\u001B[0m         \u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap\u001B[38;5;241m.\u001B[39mexterior\u001B[38;5;241m.\u001B[39mdistance(Point(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39my)))) \u001B[38;5;241m>\u001B[39m Decimal(\u001B[38;5;241m0.0\u001B[39m):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/examples/envs/navigation.py:60\u001B[0m, in \u001B[0;36mAgent.geometric_representation\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgeometric_representation\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     55\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns the shapely geometry representation of the agent\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m        shapely geometry object\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mradius\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/shapely/geometry/base.py:528\u001B[0m, in \u001B[0;36mBaseGeometry.buffer\u001B[0;34m(self, distance, quad_segs, cap_style, join_style, mitre_limit, single_sided, **kwargs)\u001B[0m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misfinite(distance)\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuffer distance must be finite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 528\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mshapely\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdistance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquad_segs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquad_segs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcap_style\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcap_style\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjoin_style\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin_style\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    534\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmitre_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmitre_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[43m    \u001B[49m\u001B[43msingle_sided\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msingle_sided\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/shapely/decorators.py:64\u001B[0m, in \u001B[0;36mmultithreading_enabled.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 64\u001B[0m     array_args \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     65\u001B[0m         arg \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m\n\u001B[1;32m     66\u001B[0m     ] \u001B[38;5;241m+\u001B[39m [\n\u001B[1;32m     67\u001B[0m         arg\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m name, arg \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhere\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, np\u001B[38;5;241m.\u001B[39mndarray)\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m\n\u001B[1;32m     72\u001B[0m     ]\n\u001B[1;32m     73\u001B[0m     old_flags \u001B[38;5;241m=\u001B[39m [arr\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m array_args]\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/shapely/decorators.py:65\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     64\u001B[0m     array_args \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m---> 65\u001B[0m         arg \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndarray\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m\n\u001B[1;32m     66\u001B[0m     ] \u001B[38;5;241m+\u001B[39m [\n\u001B[1;32m     67\u001B[0m         arg\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m name, arg \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhere\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, np\u001B[38;5;241m.\u001B[39mndarray)\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m\n\u001B[1;32m     72\u001B[0m     ]\n\u001B[1;32m     73\u001B[0m     old_flags \u001B[38;5;241m=\u001B[39m [arr\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m array_args]\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "episode_num = 0\n",
    "training_timesteps = 0\n",
    "projection = True\n",
    "\n",
    "while training_timesteps < total_timesteps:\n",
    "    restricted_environment.reset()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "    observation = None\n",
    "    action = None\n",
    "    last_td3_action = None\n",
    "\n",
    "    for agent in restricted_environment.agent_iter():\n",
    "        next_observation, reward, termination, truncation, info = restricted_environment.last()\n",
    "\n",
    "        # Turn of the agent\n",
    "        if agent == 'agent_0':\n",
    "            episode_reward += reward\n",
    "            episode_timesteps += 1\n",
    "\n",
    "            flattened_next_observation = next_observation['observation']\n",
    "\n",
    "            if episode_timesteps > 1:\n",
    "                replay_buffer.add(observation,\n",
    "                                  last_td3_action,\n",
    "                                  flattened_next_observation,\n",
    "                                  reward,\n",
    "                                  termination or truncation)\n",
    "            observation = flattened_next_observation\n",
    "\n",
    "            training_timesteps += 1\n",
    "            if training_timesteps < td3_config['train_after_timesteps']:\n",
    "                if projection:\n",
    "                    action = next_observation['restriction'].sample()\n",
    "                else:\n",
    "                    action = np.random.uniform(-110.0, 110.0, (1,))\n",
    "            else:\n",
    "                det_action = td3.select_action(observation)\n",
    "                noise = np.random.normal(0, td3_config['max_action'] * td3_config['exploration_noise'], size=td3_config['action_dim'])\n",
    "                action = (det_action + noise).clip(-td3_config['max_action'], td3_config['max_action'])\n",
    "\n",
    "                if projection:\n",
    "                    action = np.array([next_observation['restriction'].nearest_element(action[0])], dtype=np.float32)\n",
    "            if training_timesteps >= td3_config['train_after_timesteps']:\n",
    "                td3.train(replay_buffer, td3_config['batch_size'])\n",
    "            last_td3_action = action\n",
    "        # Or restrictor\n",
    "        else:\n",
    "            action = restrictor.act(next_observation)\n",
    "\n",
    "        # None action if episode is done\n",
    "        if termination or truncation:\n",
    "            # print(f'{action} for {next_observation[\"restriction\"]}, {restricted_environment.env.agent.x, restricted_environment.env.agent.y}')\n",
    "            action = None\n",
    "\n",
    "        restricted_environment.step(action)\n",
    "\n",
    "        if training_timesteps % 50 == 0:\n",
    "            print(evaluate(td3, restrictor))\n",
    "\n",
    "    print(f'Finished episode {episode_num} with reward {episode_reward} in {episode_timesteps} steps - goal reached: {termination}')\n",
    "\n",
    "restricted_environment.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
