{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from src.restrictions import IntervalUnionRestriction\n",
    "from src.wrapper import RestrictionWrapper\n",
    "from examples.agents.td3 import TD3\n",
    "from examples.envs.navigation import NavigationEnvironment\n",
    "from examples.restrictors.navigation_restrictor import NavigationRestrictor\n",
    "from examples.utils import ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Tested with seeds 45, 46, 47, 48, 49\n",
    "seed = 49\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'HEIGHT': 15.0,\n",
    "    'WIDTH': 15.0,\n",
    "    'STEPS_PER_EPISODE': 40,\n",
    "    'ACTION_RANGE': 220,\n",
    "    'DT': 1.0,\n",
    "    'TIMESTEP_PENALTY_COEFFICIENT': 0.05,\n",
    "    'REWARD_COLLISION': -1.0,\n",
    "    'REWARD_GOAL': 5.0,\n",
    "    'REWARD_COEFFICIENT': 10.0,\n",
    "    'AGENT_RADIUS': 0.5,\n",
    "    'AGENT_PERSPECTIVE': 90,\n",
    "    'AGENT_STEP_SIZE': 1.0,\n",
    "    'AGENT_X': 1.5,\n",
    "    'AGENT_Y': 1.5,\n",
    "    'GOAL_RADIUS': 1.0,\n",
    "    'GOAL_X': 12.0,\n",
    "    'GOAL_y': 12.0\n",
    "}\n",
    "environment = NavigationEnvironment(env_config)\n",
    "\n",
    "\n",
    "def projection_fn(env, action, restriction: IntervalUnionRestriction):\n",
    "    env.step(np.array([restriction.nearest_element(action[0])], dtype=np.float32))\n",
    "restrictor = NavigationRestrictor(obstacle_count=4,\n",
    "                                  obstacle_position_covariance=[[5.0, 0.0], [0.0, 5.0]],\n",
    "                                  obstacle_mean_size=1.0,\n",
    "                                  obstacle_variance_size=0.2,\n",
    "                                  obstacle_size_range=0.5,\n",
    "                                  start_seed=50,\n",
    "                                  safety_angle=8,\n",
    "                                  min_angle=-110.0,\n",
    "                                  max_angle=110.0)\n",
    "restricted_environment = RestrictionWrapper(environment, restrictor,\n",
    "                                            restriction_violation_fns=projection_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "td3_config = {\n",
    "    'state_dim': 6,\n",
    "    'action_dim': 1,\n",
    "    'max_action': 110.0,\n",
    "    'discount': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'policy_noise': 0.2,\n",
    "    'noise_clip:': 0.5,\n",
    "    'policy_freq': 2,\n",
    "    'exploration_noise': 0.2,\n",
    "    'batch_size': 256,\n",
    "    'train_after_timesteps': 2000,\n",
    "    'learning_rate_actor': 1e-5,\n",
    "    'learning_rate_critic': 1e-5\n",
    "}\n",
    "\n",
    "total_timesteps = 50000\n",
    "td3 = TD3(**td3_config)\n",
    "replay_buffer = ReplayBuffer(state_dim=td3_config['state_dim'], action_dim=td3_config['action_dim'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from src.restrictors import Restrictor\n",
    "\n",
    "\n",
    "def evaluate(eval_policy: TD3, eval_restrictor: Restrictor):\n",
    "    eval_env = RestrictionWrapper(NavigationEnvironment(env_config),\n",
    "                                  NavigationRestrictor(obstacle_count=4,\n",
    "                                                       obstacle_position_covariance=[[5.0, 0.0], [0.0, 5.0]],\n",
    "                                                       obstacle_mean_size=1.0,\n",
    "                                                       obstacle_variance_size=0.2,\n",
    "                                                       obstacle_size_range=0.5,\n",
    "                                                       start_seed=1,\n",
    "                                                       safety_angle=8,\n",
    "                                                       min_angle=-110.0,\n",
    "                                                       max_angle=110.0),\n",
    "                                  restriction_violation_fns=projection_fn)\n",
    "    eval_reward = 0.0\n",
    "\n",
    "    eval_env.reset()\n",
    "    for eval_agent in eval_env.agent_iter():\n",
    "        obs, rew, term, trunc, inf = eval_env.last()\n",
    "        if eval_agent == 'agent_0':\n",
    "            eval_reward += rew\n",
    "            eval_action = np.array([\n",
    "                obs['restriction'].nearest_element(eval_policy.select_action(obs['observation'])[0])\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            eval_action = eval_restrictor.act(obs)\n",
    "\n",
    "        if term or trunc:\n",
    "            eval_action = None\n",
    "\n",
    "        eval_env.step(eval_action)\n",
    "\n",
    "    return eval_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 1 with reward 17.102499987244716 in 24 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 2 with reward -7.000763217582208 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 3 with reward 9.183637545550695 in 41 steps - goal reached: False\n",
      "Finished episode 4 with reward -19.8016873915515 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 5 with reward 35.22606793419044 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 6 with reward 30.676594723818763 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 7 with reward 22.63403324342613 in 22 steps - goal reached: True\n",
      "Finished episode 8 with reward 22.235666572965407 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 9 with reward 25.704908850925023 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 10 with reward 75.93506544284361 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 11 with reward 19.696671780288803 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 12 with reward -3.7963357239488387 in 41 steps - goal reached: False\n",
      "Finished episode 13 with reward 36.40705934060024 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 14 with reward -46.519063286515916 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 15 with reward -22.68953166875103 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 16 with reward 47.881449904797556 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 17 with reward 32.418007958720594 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 18 with reward -18.646689953260672 in 41 steps - goal reached: False\n",
      "Finished episode 19 with reward -10.017963854152 in 3 steps - goal reached: True\n",
      "Finished episode 20 with reward 121.96081155520109 in 27 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 21 with reward 65.92406349650051 in 41 steps - goal reached: False\n",
      "Finished episode 22 with reward -20.30836277305034 in 23 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 23 with reward 15.554474541733969 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 24 with reward 60.29376827152727 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 25 with reward 28.54661766332177 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 26 with reward 12.864620119215854 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 27 with reward 29.31822274896303 in 41 steps - goal reached: False\n",
      "Finished episode 28 with reward 10.785829220566727 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 29 with reward 63.00268734008711 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 30 with reward 28.254165105407637 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 31 with reward 36.46253067251061 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 32 with reward 5.46047293351841 in 41 steps - goal reached: False\n",
      "Finished episode 33 with reward 81.57411646378213 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 34 with reward 47.60748580316113 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 35 with reward -1.831022342827687 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 36 with reward -36.12088698796381 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 37 with reward -14.642296093933947 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 38 with reward 3.0115364129391153 in 41 steps - goal reached: False\n",
      "Finished episode 39 with reward 10.278359818416835 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 40 with reward -15.049951775276005 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 41 with reward 4.562600775927188 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 42 with reward 14.205849725831861 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 43 with reward 40.33587073001615 in 41 steps - goal reached: False\n",
      "Finished episode 44 with reward 24.085452068906392 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 45 with reward 6.773037000828046 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 46 with reward 58.662482616152275 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 47 with reward -31.717178865212105 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 48 with reward 31.531400538694516 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 49 with reward 43.208925396694376 in 41 steps - goal reached: False\n",
      "Finished episode 50 with reward 47.63521694849068 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 51 with reward 103.79639990388995 in 36 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 52 with reward 16.660525903361766 in 41 steps - goal reached: False\n",
      "Finished episode 53 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 54 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.12158948958428\n",
      "Evaluation result: -10.12158948958428\n",
      "Finished episode 55 with reward 46.91240284013932 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12159365074526\n",
      "Evaluation result: -10.12159365074526\n",
      "Finished episode 56 with reward 80.89721471563182 in 35 steps - goal reached: True\n",
      "Finished episode 57 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 58 with reward 75.23288151048887 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12158602194289\n",
      "Finished episode 59 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.12158602194289\n",
      "Finished episode 60 with reward 114.06890442549931 in 31 steps - goal reached: True\n",
      "Finished episode 61 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 62 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.1215333129852\n",
      "Evaluation result: -10.1215333129852\n",
      "Finished episode 63 with reward 39.675365694884114 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.0964882361568\n",
      "Evaluation result: -10.0964882361568\n",
      "Finished episode 64 with reward 0.5692666938877764 in 41 steps - goal reached: False\n",
      "Evaluation result: 18.64304058694765\n",
      "Evaluation result: 18.64304058694765\n",
      "Finished episode 65 with reward 74.03545603378143 in 33 steps - goal reached: True\n",
      "Finished episode 66 with reward 104.84437223888266 in 34 steps - goal reached: True\n",
      "Evaluation result: 58.0369801377655\n",
      "Evaluation result: 92.75840362030681\n",
      "Finished episode 67 with reward 117.82974522726148 in 30 steps - goal reached: True\n",
      "Evaluation result: 74.66448886423669\n",
      "Evaluation result: 74.66448886423669\n",
      "Finished episode 68 with reward 82.16464580925576 in 41 steps - goal reached: False\n",
      "Finished episode 69 with reward 120.91024794527465 in 29 steps - goal reached: True\n",
      "Evaluation result: 111.89775687868301\n",
      "Evaluation result: 105.48321590655286\n",
      "Finished episode 70 with reward 103.47241575497915 in 35 steps - goal reached: True\n",
      "Evaluation result: 38.12186304066033\n",
      "Evaluation result: 100.07132399970425\n",
      "Finished episode 71 with reward 109.56197625078491 in 36 steps - goal reached: True\n",
      "Finished episode 72 with reward 117.61640808367743 in 24 steps - goal reached: True\n",
      "Evaluation result: 93.61469244859069\n",
      "Evaluation result: 103.05885736994753\n",
      "Finished episode 73 with reward 126.38686152144491 in 24 steps - goal reached: True\n",
      "Finished episode 74 with reward 79.49879914365647 in 41 steps - goal reached: False\n",
      "Evaluation result: 96.20939782687229\n",
      "Evaluation result: 96.20939782687229\n",
      "Finished episode 75 with reward 106.71973674290332 in 34 steps - goal reached: True\n",
      "Evaluation result: 121.30656379861\n",
      "Evaluation result: 109.96699464045236\n",
      "Finished episode 76 with reward 104.93352087704424 in 34 steps - goal reached: True\n",
      "Finished episode 77 with reward 110.1251514978647 in 35 steps - goal reached: True\n",
      "Evaluation result: 108.69282677352984\n",
      "Evaluation result: 108.69282677352984\n",
      "Finished episode 78 with reward 111.07244698444997 in 33 steps - goal reached: True\n",
      "Evaluation result: 117.88948059410637\n",
      "Evaluation result: 117.88948059410637\n",
      "Finished episode 79 with reward 105.40084141716673 in 37 steps - goal reached: True\n",
      "Finished episode 80 with reward 112.9788194882565 in 29 steps - goal reached: True\n",
      "Evaluation result: 118.58929537310753\n",
      "Evaluation result: 115.0869121189449\n",
      "Finished episode 81 with reward 122.9647326568012 in 26 steps - goal reached: True\n",
      "Evaluation result: 114.65851998810126\n",
      "Finished episode 82 with reward 120.38653850235498 in 27 steps - goal reached: True\n",
      "Finished episode 83 with reward 100.53731016703993 in 38 steps - goal reached: True\n",
      "Evaluation result: 117.91394661956836\n",
      "Evaluation result: 117.91394661956836\n",
      "Finished episode 84 with reward 111.5073342766002 in 33 steps - goal reached: True\n",
      "Finished episode 85 with reward 114.74178979813287 in 27 steps - goal reached: True\n",
      "Evaluation result: 80.55463703011844\n",
      "Evaluation result: 122.79843788137907\n",
      "Finished episode 86 with reward 124.88124177218947 in 24 steps - goal reached: True\n",
      "Evaluation result: 115.42840726437424\n",
      "Finished episode 87 with reward 120.8867410508372 in 27 steps - goal reached: True\n",
      "Evaluation result: 115.42840726437424\n",
      "Finished episode 88 with reward 100.5972616408361 in 40 steps - goal reached: True\n",
      "Evaluation result: 116.1780191297477\n",
      "Evaluation result: 116.1780191297477\n",
      "Finished episode 89 with reward 118.15675582809827 in 30 steps - goal reached: True\n",
      "Finished episode 90 with reward 120.3156383539631 in 29 steps - goal reached: True\n",
      "Evaluation result: 112.06347102482898\n",
      "Evaluation result: 112.06347102482898\n",
      "Finished episode 91 with reward 99.4824450937077 in 41 steps - goal reached: True\n",
      "Evaluation result: 112.14245233067315\n",
      "Evaluation result: 114.36825265644804\n",
      "Finished episode 92 with reward 117.39838573246288 in 31 steps - goal reached: True\n",
      "Finished episode 93 with reward 122.14128157299315 in 27 steps - goal reached: True\n",
      "Evaluation result: 112.4267743690497\n",
      "Evaluation result: 126.12855353811815\n",
      "Finished episode 94 with reward 119.63719485610594 in 28 steps - goal reached: True\n",
      "Evaluation result: 109.32979360793912\n",
      "Evaluation result: 104.80487243612235\n",
      "Finished episode 95 with reward 116.37224542722275 in 29 steps - goal reached: True\n",
      "Finished episode 96 with reward 121.2175060430918 in 25 steps - goal reached: True\n",
      "Evaluation result: 113.05959136782377\n",
      "Evaluation result: 112.24839373071634\n",
      "Finished episode 97 with reward 117.99994351752018 in 28 steps - goal reached: True\n",
      "Finished episode 98 with reward 106.67293438394057 in 36 steps - goal reached: True\n",
      "Evaluation result: 101.33352884651886\n",
      "Evaluation result: 103.78352722057535\n",
      "Finished episode 99 with reward 120.41168622337045 in 28 steps - goal reached: True\n",
      "Evaluation result: 117.31136207634123\n",
      "Evaluation result: 39.07727507744\n",
      "Finished episode 100 with reward 94.42070010737699 in 41 steps - goal reached: False\n",
      "Finished episode 101 with reward 119.28352534851584 in 27 steps - goal reached: True\n",
      "Evaluation result: 113.6788915598604\n",
      "Evaluation result: 114.19423398091104\n",
      "Finished episode 102 with reward 117.6869681993065 in 31 steps - goal reached: True\n",
      "Evaluation result: 118.55488398225003\n",
      "Evaluation result: 119.49215252967834\n",
      "Finished episode 103 with reward 103.34989125452186 in 38 steps - goal reached: True\n",
      "Finished episode 104 with reward 122.21445914017374 in 28 steps - goal reached: True\n",
      "Evaluation result: 116.72539954631551\n",
      "Evaluation result: 109.70905892823204\n",
      "Finished episode 105 with reward 117.18663386506113 in 28 steps - goal reached: True\n",
      "Finished episode 106 with reward 121.88738816514987 in 28 steps - goal reached: True\n",
      "Evaluation result: 120.62123477062721\n",
      "Evaluation result: 116.44846392043489\n",
      "Finished episode 107 with reward 117.9969907831894 in 26 steps - goal reached: True\n",
      "Finished episode 108 with reward 120.18081165610268 in 25 steps - goal reached: True\n",
      "Evaluation result: 102.89499742205595\n",
      "Evaluation result: 111.58216931478101\n",
      "Finished episode 109 with reward 120.34465195835958 in 26 steps - goal reached: True\n",
      "Evaluation result: 78.952176318539\n",
      "Finished episode 110 with reward 118.75762195865347 in 30 steps - goal reached: True\n",
      "Evaluation result: 75.30368516460864\n",
      "Finished episode 111 with reward 84.0424653628579 in 41 steps - goal reached: False\n",
      "Evaluation result: 110.96791120796823\n",
      "Evaluation result: 110.96791120796823\n",
      "Finished episode 112 with reward 124.51001996575906 in 23 steps - goal reached: True\n",
      "Finished episode 113 with reward 111.34902512945332 in 31 steps - goal reached: True\n",
      "Evaluation result: 44.985717843295745\n",
      "Evaluation result: 41.30935111946562\n",
      "Finished episode 114 with reward 124.02335487359841 in 26 steps - goal reached: True\n",
      "Evaluation result: 117.46487002498807\n",
      "Finished episode 115 with reward 116.59319269777384 in 29 steps - goal reached: True\n",
      "Evaluation result: 117.46487002498807\n",
      "Finished episode 116 with reward 114.18986672656362 in 30 steps - goal reached: True\n",
      "Evaluation result: 118.70556568343325\n",
      "Evaluation result: 105.05092294459337\n",
      "Finished episode 117 with reward 116.17677701623688 in 28 steps - goal reached: True\n",
      "Finished episode 118 with reward 118.04001132379743 in 31 steps - goal reached: True\n",
      "Evaluation result: 107.48815116540536\n",
      "Evaluation result: 107.48815116540536\n",
      "Finished episode 119 with reward 119.10355485619623 in 27 steps - goal reached: True\n",
      "Finished episode 120 with reward 115.73596217507563 in 26 steps - goal reached: True\n",
      "Evaluation result: 103.73564443092961\n",
      "Evaluation result: 120.46649804525366\n",
      "Finished episode 121 with reward 120.94066993112764 in 25 steps - goal reached: True\n",
      "Finished episode 122 with reward 119.72093426969504 in 26 steps - goal reached: True\n",
      "Evaluation result: 79.24897089150944\n",
      "Evaluation result: 106.6499577225345\n",
      "Finished episode 123 with reward 118.39797504479226 in 25 steps - goal reached: True\n",
      "Finished episode 124 with reward 123.42841701430687 in 26 steps - goal reached: True\n",
      "Evaluation result: 114.77849210550407\n",
      "Evaluation result: 108.03927440788102\n",
      "Finished episode 125 with reward 123.26682748742975 in 26 steps - goal reached: True\n",
      "Finished episode 126 with reward 112.66740871716739 in 29 steps - goal reached: True\n",
      "Evaluation result: 81.96719949101157\n",
      "Evaluation result: 110.20178377203901\n",
      "Finished episode 127 with reward 115.35171414685209 in 26 steps - goal reached: True\n",
      "Evaluation result: 117.07230442162428\n",
      "Evaluation result: 119.31741482194704\n",
      "Finished episode 128 with reward 116.21940602117392 in 29 steps - goal reached: True\n",
      "Finished episode 129 with reward 120.15006111102899 in 24 steps - goal reached: True\n",
      "Evaluation result: 110.47893479752275\n",
      "Evaluation result: 110.47893479752275\n",
      "Finished episode 130 with reward 118.73674938894223 in 26 steps - goal reached: True\n",
      "Finished episode 131 with reward 123.30244145969817 in 23 steps - goal reached: True\n",
      "Evaluation result: 84.91592692176624\n",
      "Evaluation result: 84.91592692176624\n",
      "Finished episode 132 with reward 100.61080021677245 in 38 steps - goal reached: True\n",
      "Finished episode 133 with reward 122.8117654138414 in 25 steps - goal reached: True\n",
      "Evaluation result: 108.69546213405141\n",
      "Evaluation result: 108.69546213405141\n",
      "Finished episode 134 with reward 83.94583834340392 in 41 steps - goal reached: False\n",
      "Evaluation result: 118.54774082577967\n",
      "Evaluation result: 104.03559961123221\n",
      "Finished episode 135 with reward 123.14565809562966 in 26 steps - goal reached: True\n",
      "Finished episode 136 with reward 106.32604645600573 in 36 steps - goal reached: True\n",
      "Evaluation result: 107.03600766217872\n",
      "Evaluation result: 107.03600766217872\n",
      "Finished episode 137 with reward 124.19647187149393 in 25 steps - goal reached: True\n",
      "Finished episode 138 with reward 125.55361303652825 in 24 steps - goal reached: True\n",
      "Evaluation result: 119.3874252946415\n",
      "Evaluation result: 98.59500850766213\n",
      "Finished episode 139 with reward 120.74547806065898 in 24 steps - goal reached: True\n",
      "Evaluation result: 115.26589988893589\n",
      "Evaluation result: 115.26589988893589\n",
      "Finished episode 140 with reward 84.3633436071642 in 41 steps - goal reached: False\n",
      "Finished episode 141 with reward 120.69168819792789 in 25 steps - goal reached: True\n",
      "Evaluation result: 124.64682792093635\n",
      "Evaluation result: 72.84707361449418\n",
      "Finished episode 142 with reward 118.52587877040126 in 26 steps - goal reached: True\n",
      "Finished episode 143 with reward 122.23826332849794 in 28 steps - goal reached: True\n",
      "Evaluation result: 39.02449663687315\n",
      "Evaluation result: 39.02449663687315\n",
      "Finished episode 144 with reward 117.47125774160763 in 26 steps - goal reached: True\n",
      "Finished episode 145 with reward 120.56175436477913 in 25 steps - goal reached: True\n",
      "Evaluation result: 126.21912537190865\n",
      "Evaluation result: 107.77323789210968\n",
      "Finished episode 146 with reward 122.28797562383257 in 26 steps - goal reached: True\n",
      "Evaluation result: 118.77109361886\n",
      "Finished episode 147 with reward 105.27396080093293 in 37 steps - goal reached: True\n",
      "Evaluation result: 118.77109361886\n",
      "Finished episode 148 with reward 120.25784700287062 in 26 steps - goal reached: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 48\u001B[0m\n\u001B[1;32m     45\u001B[0m     last_td3_action \u001B[38;5;241m=\u001B[39m action\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Or restrictor\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mrestrictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_observation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# None action if episode is done\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m termination \u001B[38;5;129;01mor\u001B[39;00m truncation:\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;66;03m# print(f'{action} for {next_observation[\"restriction\"]}, {restricted_environment.env.agent.x, restricted_environment.env.agent.y}')\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/examples/restrictors/navigation_restrictor.py:321\u001B[0m, in \u001B[0;36mNavigationRestrictor.act\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obstacle, Obstacle):\n\u001B[1;32m    318\u001B[0m     obstacle \u001B[38;5;241m=\u001B[39m obstacle\u001B[38;5;241m.\u001B[39mcollision_area(\u001B[38;5;28mfloat\u001B[39m(agent\u001B[38;5;241m.\u001B[39mradius))\n\u001B[1;32m    320\u001B[0m is_in_collision_area \u001B[38;5;241m=\u001B[39m obstacle\u001B[38;5;241m.\u001B[39mcontains(\n\u001B[0;32m--> 321\u001B[0m     Point(\u001B[38;5;28mfloat\u001B[39m(agent\u001B[38;5;241m.\u001B[39mx), \u001B[38;5;28mfloat\u001B[39m(agent\u001B[38;5;241m.\u001B[39my))) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mobstacle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mboundary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontains\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mPoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    324\u001B[0m obstacle_step_circle_intersection \u001B[38;5;241m=\u001B[39m step_circle\u001B[38;5;241m.\u001B[39mintersection(\n\u001B[1;32m    325\u001B[0m     obstacle) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_in_collision_area \u001B[38;5;28;01melse\u001B[39;00m (\n\u001B[1;32m    326\u001B[0m     step_circle\u001B[38;5;241m.\u001B[39mboundary\u001B[38;5;241m.\u001B[39mdifference(obstacle))\n\u001B[1;32m    328\u001B[0m \u001B[38;5;66;03m# If intersection consists of multiple parts, iterate through them\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/shapely/geometry/base.py:658\u001B[0m, in \u001B[0;36mBaseGeometry.contains\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontains\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[1;32m    657\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns True if the geometry contains the other, else False\"\"\"\u001B[39;00m\n\u001B[0;32m--> 658\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _maybe_unpack(\u001B[43mshapely\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontains\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/shapely/decorators.py:77\u001B[0m, in \u001B[0;36mmultithreading_enabled.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m array_args:\n\u001B[1;32m     76\u001B[0m         arr\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m arr, old_flag \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(array_args, old_flags):\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/shapely/predicates.py:540\u001B[0m, in \u001B[0;36mcontains\u001B[0;34m(a, b, **kwargs)\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;129m@multithreading_enabled\u001B[39m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontains\u001B[39m(a, b, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    487\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns True if geometry B is completely inside geometry A.\u001B[39;00m\n\u001B[1;32m    488\u001B[0m \n\u001B[1;32m    489\u001B[0m \u001B[38;5;124;03m    A contains B if no points of B lie in the exterior of A and at least one\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontains\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "episode_num = 0\n",
    "training_timesteps = 0\n",
    "sample_from_restricted_space = True\n",
    "\n",
    "while training_timesteps < total_timesteps:\n",
    "    restricted_environment.reset()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "    observation = None\n",
    "    action = None\n",
    "    last_td3_action = None\n",
    "\n",
    "    for agent in restricted_environment.agent_iter():\n",
    "        next_observation, reward, termination, truncation, info = restricted_environment.last()\n",
    "\n",
    "        # Turn of the agent\n",
    "        if agent == 'agent_0':\n",
    "            episode_reward += reward\n",
    "            episode_timesteps += 1\n",
    "\n",
    "            flattened_next_observation = next_observation['observation']\n",
    "\n",
    "            if episode_timesteps > 1:\n",
    "                replay_buffer.add(observation,\n",
    "                                  last_td3_action,\n",
    "                                  flattened_next_observation,\n",
    "                                  reward,\n",
    "                                  termination or truncation)\n",
    "            observation = flattened_next_observation\n",
    "\n",
    "            training_timesteps += 1\n",
    "            if training_timesteps < td3_config['train_after_timesteps']:\n",
    "                if sample_from_restricted_space:\n",
    "                    action = next_observation['restriction'].sample()\n",
    "                else:\n",
    "                    action = np.random.uniform(-110.0, 110.0, (1,))\n",
    "            else:\n",
    "                det_action = td3.select_action(observation)\n",
    "                noise = np.random.normal(0, td3_config['max_action'] * td3_config['exploration_noise'], size=td3_config['action_dim'])\n",
    "                action = (det_action + noise).clip(-td3_config['max_action'], td3_config['max_action'])\n",
    "\n",
    "            if training_timesteps >= td3_config['train_after_timesteps']:\n",
    "                td3.train(replay_buffer, td3_config['batch_size'])\n",
    "            last_td3_action = action\n",
    "        # Or restrictor\n",
    "        else:\n",
    "            action = restrictor.act(next_observation)\n",
    "\n",
    "        # None action if episode is done\n",
    "        if termination or truncation:\n",
    "            # print(f'{action} for {next_observation[\"restriction\"]}, {restricted_environment.env.agent.x, restricted_environment.env.agent.y}')\n",
    "            action = None\n",
    "\n",
    "        restricted_environment.step(action)\n",
    "\n",
    "        if training_timesteps % 50 == 0:\n",
    "            print(f'Evaluation result: {evaluate(td3, restrictor)}')\n",
    "\n",
    "    print(f'Finished episode {episode_num} with reward {episode_reward} in {episode_timesteps} steps - goal reached: {termination}')\n",
    "\n",
    "restricted_environment.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
