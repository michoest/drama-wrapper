{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from src.restrictions import IntervalUnionRestriction\n",
    "from src.wrapper import RestrictionWrapper\n",
    "from examples.agents.td3 import TD3\n",
    "from examples.envs.navigation import NavigationEnvironment\n",
    "from examples.restrictors.navigation_restrictor import NavigationRestrictor\n",
    "from examples.utils import ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Tested with seeds 45, 46, 47, 48, 49\n",
    "seed = 49\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'HEIGHT': 15.0,\n",
    "    'WIDTH': 15.0,\n",
    "    'STEPS_PER_EPISODE': 40,\n",
    "    'ACTION_RANGE': 220,\n",
    "    'DT': 1.0,\n",
    "    'TIMESTEP_PENALTY_COEFFICIENT': 0.05,\n",
    "    'REWARD_COLLISION': -1.0,\n",
    "    'REWARD_GOAL': 5.0,\n",
    "    'REWARD_COEFFICIENT': 10.0,\n",
    "    'AGENT_RADIUS': 0.5,\n",
    "    'AGENT_PERSPECTIVE': 90,\n",
    "    'AGENT_STEP_SIZE': 1.0,\n",
    "    'AGENT_X': 1.5,\n",
    "    'AGENT_Y': 1.5,\n",
    "    'GOAL_RADIUS': 1.0,\n",
    "    'GOAL_X': 12.0,\n",
    "    'GOAL_y': 12.0\n",
    "}\n",
    "environment = NavigationEnvironment(env_config)\n",
    "\n",
    "def projection_fn(env, action, restriction: IntervalUnionRestriction):\n",
    "    return np.array([restriction.nearest_element(action[0])], dtype=np.float32)\n",
    "restrictor = NavigationRestrictor(obstacle_count=4,\n",
    "                                  obstacle_position_covariance=[[5.0, 0.0], [0.0, 5.0]],\n",
    "                                  obstacle_mean_size=1.0,\n",
    "                                  obstacle_variance_size=0.2,\n",
    "                                  obstacle_size_range=0.5,\n",
    "                                  start_seed=50,\n",
    "                                  safety_angle=8,\n",
    "                                  min_angle=-110.0,\n",
    "                                  max_angle=110.0)\n",
    "restricted_environment = RestrictionWrapper(environment, restrictor,\n",
    "                                            restriction_violation_fns=projection_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "td3_config = {\n",
    "    'state_dim': 6,\n",
    "    'action_dim': 1,\n",
    "    'max_action': 110.0,\n",
    "    'discount': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'policy_noise': 0.2,\n",
    "    'noise_clip:': 0.5,\n",
    "    'policy_freq': 2,\n",
    "    'exploration_noise': 0.2,\n",
    "    'batch_size': 256,\n",
    "    'train_after_timesteps': 2000,\n",
    "    'learning_rate_actor': 1e-5,\n",
    "    'learning_rate_critic': 1e-5\n",
    "}\n",
    "\n",
    "total_timesteps = 50000\n",
    "td3 = TD3(**td3_config)\n",
    "replay_buffer = ReplayBuffer(state_dim=td3_config['state_dim'], action_dim=td3_config['action_dim'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from src.restrictors import Restrictor\n",
    "\n",
    "\n",
    "def evaluate(eval_policy: TD3, eval_restrictor: Restrictor):\n",
    "    eval_env = RestrictionWrapper(NavigationEnvironment(env_config),\n",
    "                                  NavigationRestrictor(obstacle_count=4,\n",
    "                                                       obstacle_position_covariance=[[5.0, 0.0], [0.0, 5.0]],\n",
    "                                                       obstacle_mean_size=1.0,\n",
    "                                                       obstacle_variance_size=0.2,\n",
    "                                                       obstacle_size_range=0.5,\n",
    "                                                       start_seed=1,\n",
    "                                                       safety_angle=8,\n",
    "                                                       min_angle=-110.0,\n",
    "                                                       max_angle=110.0),\n",
    "                                  restriction_violation_fns=projection_fn)\n",
    "    eval_reward = 0.0\n",
    "\n",
    "    eval_env.reset()\n",
    "    for eval_agent in eval_env.agent_iter():\n",
    "        obs, rew, term, trunc, inf = eval_env.last()\n",
    "        if eval_agent == 'agent_0':\n",
    "            eval_reward += rew\n",
    "            eval_action = np.array([\n",
    "                obs['restriction'].nearest_element(eval_policy.select_action(obs['observation'])[0])\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            eval_action = eval_restrictor.act(obs)\n",
    "\n",
    "        if term or trunc:\n",
    "            eval_action = None\n",
    "\n",
    "        eval_env.step(eval_action)\n",
    "\n",
    "    return eval_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 1 with reward 17.102499987244716 in 24 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 2 with reward -7.000763217582208 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 3 with reward 9.183637545550695 in 41 steps - goal reached: False\n",
      "Finished episode 4 with reward -19.8016873915515 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 5 with reward 35.22606793419044 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 6 with reward 30.676594723818763 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 7 with reward 53.44931874593792 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 8 with reward 80.53232289338621 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 9 with reward 54.5229918767436 in 41 steps - goal reached: False\n",
      "Finished episode 10 with reward -19.577695761764303 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 11 with reward -24.490159319368786 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 12 with reward -4.069304219843656 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 13 with reward 32.79183321922926 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 14 with reward 21.8771613945973 in 41 steps - goal reached: False\n",
      "Finished episode 15 with reward -16.860199412174147 in 17 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 16 with reward 10.386484843824027 in 28 steps - goal reached: True\n",
      "Finished episode 17 with reward 47.08579389695376 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 18 with reward -4.960075238236616 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 19 with reward 58.13160512481784 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 20 with reward 42.02700977392242 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 21 with reward 1.7412957771907962 in 41 steps - goal reached: False\n",
      "Finished episode 22 with reward 1.06688057516563 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 23 with reward 44.340378759382155 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 24 with reward -7.7669310224083015 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 25 with reward -4.9212436512152085 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 26 with reward 1.8191100810194696 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 27 with reward 73.280022288413 in 41 steps - goal reached: False\n",
      "Finished episode 28 with reward 3.5809106929020853 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 29 with reward 15.344647690419865 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 30 with reward 21.056691047334503 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 31 with reward -20.380465772235798 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 32 with reward 8.955036992783697 in 41 steps - goal reached: False\n",
      "Finished episode 33 with reward 6.665108084260996 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 34 with reward 21.73208991682609 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 35 with reward 98.13253601417598 in 39 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 36 with reward 15.970585356285135 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 37 with reward -6.529882081063655 in 41 steps - goal reached: False\n",
      "Finished episode 38 with reward 53.718819354184284 in 38 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 39 with reward 14.045523679336654 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 40 with reward 29.926872920095228 in 41 steps - goal reached: False\n",
      "Finished episode 41 with reward -10.10139132295976 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 42 with reward 21.83237874867915 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 43 with reward 20.973746048199413 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 44 with reward -1.5137529649638326 in 41 steps - goal reached: False\n",
      "Finished episode 45 with reward 3.969959920372115 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 46 with reward 121.4314158703504 in 24 steps - goal reached: True\n",
      "Finished episode 47 with reward -2.1933544166437002 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 48 with reward -8.365561107110162 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 49 with reward 83.87000947667767 in 41 steps - goal reached: False\n",
      "Finished episode 50 with reward -46.26362028864228 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 51 with reward 44.23918416053113 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 52 with reward -3.8736566633396974 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12133494725474\n",
      "Evaluation result: -10.12133494725474\n",
      "Finished episode 53 with reward -25.733897011398298 in 41 steps - goal reached: False\n",
      "Finished episode 54 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 55 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.12159781189681\n",
      "Evaluation result: -10.12159781189681\n",
      "Finished episode 56 with reward 113.48414635064928 in 33 steps - goal reached: True\n",
      "Finished episode 57 with reward 121.28389203143625 in 28 steps - goal reached: True\n",
      "Evaluation result: -10.12160127951585\n",
      "Evaluation result: -10.12160127951585\n",
      "Finished episode 58 with reward 29.545010872796244 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12160058599257\n",
      "Evaluation result: -10.12160058599257\n",
      "Finished episode 59 with reward 36.648386330087156 in 41 steps - goal reached: False\n",
      "Evaluation result: -10.12159850542114\n",
      "Evaluation result: -10.12159850542114\n",
      "Finished episode 60 with reward 0.5756969220188832 in 41 steps - goal reached: False\n",
      "Finished episode 61 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 62 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Finished episode 63 with reward 90.00848558573371 in 25 steps - goal reached: True\n",
      "Finished episode 64 with reward -10.12160474712833 in 3 steps - goal reached: True\n",
      "Evaluation result: -10.12155966765376\n",
      "Evaluation result: -10.12155966765376\n",
      "Finished episode 65 with reward 112.56596609606706 in 33 steps - goal reached: True\n",
      "Finished episode 66 with reward 10.22126589378362 in 6 steps - goal reached: True\n",
      "Evaluation result: -4.100851426772941\n",
      "Evaluation result: 71.68381972137409\n",
      "Finished episode 67 with reward 69.90064954339523 in 22 steps - goal reached: True\n",
      "Finished episode 68 with reward 17.151315566200964 in 41 steps - goal reached: False\n",
      "Evaluation result: 43.74889397875476\n",
      "Evaluation result: 43.74889397875476\n",
      "Finished episode 69 with reward 26.433124350159854 in 41 steps - goal reached: False\n",
      "Evaluation result: 84.12247644940787\n",
      "Evaluation result: 38.18709871398241\n",
      "Finished episode 70 with reward 100.91207919601356 in 38 steps - goal reached: True\n",
      "Evaluation result: 105.70416408140083\n",
      "Evaluation result: 105.70416408140083\n",
      "Finished episode 71 with reward 85.32383399599347 in 41 steps - goal reached: False\n",
      "Finished episode 72 with reward 123.52344380068502 in 25 steps - goal reached: True\n",
      "Evaluation result: 106.8450315295057\n",
      "Evaluation result: 102.07657311926111\n",
      "Finished episode 73 with reward 76.08754076971468 in 41 steps - goal reached: False\n",
      "Evaluation result: 114.86429623632611\n",
      "Evaluation result: 114.86429623632611\n",
      "Finished episode 74 with reward 120.6771485095072 in 29 steps - goal reached: True\n",
      "Finished episode 75 with reward 118.03141982384918 in 29 steps - goal reached: True\n",
      "Evaluation result: 124.2254710187812\n",
      "Evaluation result: 115.53537398237735\n",
      "Finished episode 76 with reward 101.08078648057467 in 41 steps - goal reached: True\n",
      "Evaluation result: 120.46676738056175\n",
      "Evaluation result: 120.46676738056175\n",
      "Finished episode 77 with reward 111.21278598345727 in 35 steps - goal reached: True\n",
      "Finished episode 78 with reward 118.46323817566855 in 28 steps - goal reached: True\n",
      "Evaluation result: 117.91017796565853\n",
      "Evaluation result: 117.91017796565853\n",
      "Finished episode 79 with reward 119.80704596284097 in 26 steps - goal reached: True\n",
      "Finished episode 80 with reward 118.39279147233673 in 28 steps - goal reached: True\n",
      "Evaluation result: 118.0429005255263\n",
      "Evaluation result: 114.57720814392215\n",
      "Finished episode 81 with reward 116.22034437110203 in 30 steps - goal reached: True\n",
      "Evaluation result: 112.92601235229162\n",
      "Evaluation result: 117.07299826205163\n",
      "Finished episode 82 with reward 121.94588083270607 in 25 steps - goal reached: True\n",
      "Finished episode 83 with reward 105.99912285815019 in 35 steps - goal reached: True\n",
      "Evaluation result: 112.85596192581171\n",
      "Evaluation result: 112.85596192581171\n",
      "Finished episode 84 with reward 99.60275128851704 in 37 steps - goal reached: True\n",
      "Evaluation result: 111.79553841192987\n",
      "Evaluation result: 122.75203351815716\n",
      "Finished episode 85 with reward 45.478204699898015 in 41 steps - goal reached: False\n",
      "Finished episode 86 with reward 118.23780587558468 in 29 steps - goal reached: True\n",
      "Evaluation result: 74.60904162335447\n",
      "Evaluation result: 74.60904162335447\n",
      "Finished episode 87 with reward 123.62700407355011 in 23 steps - goal reached: True\n",
      "Evaluation result: 117.9685758645595\n",
      "Evaluation result: 117.9685758645595\n",
      "Finished episode 88 with reward 114.030205368867 in 33 steps - goal reached: True\n",
      "Finished episode 89 with reward 121.31511608042426 in 24 steps - goal reached: True\n",
      "Evaluation result: 91.94608089759357\n",
      "Evaluation result: 91.94608089759357\n",
      "Finished episode 90 with reward 120.63228785244036 in 27 steps - goal reached: True\n",
      "Finished episode 91 with reward 118.73917714738695 in 25 steps - goal reached: True\n",
      "Evaluation result: 114.45506605347572\n",
      "Evaluation result: 118.51633846859802\n",
      "Finished episode 92 with reward 118.70451523766387 in 27 steps - goal reached: True\n",
      "Finished episode 93 with reward 117.2075755907691 in 27 steps - goal reached: True\n",
      "Evaluation result: 126.05018720315736\n",
      "Evaluation result: 119.14049883352455\n",
      "Finished episode 94 with reward 115.36477465882776 in 26 steps - goal reached: True\n",
      "Finished episode 95 with reward 119.24668044636843 in 27 steps - goal reached: True\n",
      "Evaluation result: 118.17497174661175\n",
      "Evaluation result: 107.2917372348764\n",
      "Finished episode 96 with reward 120.18628124744244 in 28 steps - goal reached: True\n",
      "Finished episode 97 with reward 123.00115891467773 in 24 steps - goal reached: True\n",
      "Evaluation result: 114.09952098664228\n",
      "Evaluation result: 122.5237806042655\n",
      "Finished episode 98 with reward 125.36267758443725 in 25 steps - goal reached: True\n",
      "Evaluation result: 105.10911188860538\n",
      "Evaluation result: 59.10756470804267\n",
      "Finished episode 99 with reward 76.9743240689744 in 41 steps - goal reached: False\n",
      "Finished episode 100 with reward 123.53297442004373 in 26 steps - goal reached: True\n",
      "Evaluation result: 114.70594491749844\n",
      "Evaluation result: 114.70594491749844\n",
      "Finished episode 101 with reward 115.94078359153777 in 26 steps - goal reached: True\n",
      "Finished episode 102 with reward 123.42919370554672 in 24 steps - goal reached: True\n",
      "Evaluation result: 119.8460558217509\n",
      "Evaluation result: 116.63436916504982\n",
      "Finished episode 103 with reward 121.00563984043679 in 25 steps - goal reached: True\n",
      "Finished episode 104 with reward 120.64299713364206 in 29 steps - goal reached: True\n",
      "Evaluation result: 108.33235223550498\n",
      "Evaluation result: 108.33235223550498\n",
      "Finished episode 105 with reward 117.81014935707105 in 27 steps - goal reached: True\n",
      "Evaluation result: 121.69549898133133\n",
      "Finished episode 106 with reward 21.788674479952842 in 41 steps - goal reached: False\n",
      "Finished episode 107 with reward 119.28691392678627 in 26 steps - goal reached: True\n",
      "Evaluation result: 111.71619143371959\n",
      "Evaluation result: 111.71619143371959\n",
      "Finished episode 108 with reward -6.536935297866915 in 41 steps - goal reached: False\n",
      "Evaluation result: 96.22656278039325\n",
      "Evaluation result: 115.65413484183496\n",
      "Finished episode 109 with reward 85.6379462188625 in 41 steps - goal reached: False\n",
      "Finished episode 110 with reward 121.94813906602627 in 25 steps - goal reached: True\n",
      "Evaluation result: 112.72677944086234\n",
      "Evaluation result: 121.32497142840585\n",
      "Finished episode 111 with reward 121.79054474613127 in 25 steps - goal reached: True\n",
      "Finished episode 112 with reward 113.45676891215975 in 28 steps - goal reached: True\n",
      "Evaluation result: 77.53375446663362\n",
      "Evaluation result: 85.79470876655672\n",
      "Finished episode 113 with reward 116.04529246033835 in 31 steps - goal reached: True\n",
      "Finished episode 114 with reward 123.63695551079155 in 26 steps - goal reached: True\n",
      "Evaluation result: 76.83611910321663\n",
      "Evaluation result: 76.83611910321663\n",
      "Finished episode 115 with reward 117.3080234985215 in 31 steps - goal reached: True\n",
      "Evaluation result: 112.98615519885125\n",
      "Evaluation result: 112.98615519885125\n",
      "Finished episode 116 with reward 114.78752775767455 in 28 steps - goal reached: True\n",
      "Finished episode 117 with reward 116.17713410699615 in 27 steps - goal reached: True\n",
      "Evaluation result: 107.56517233166876\n",
      "Evaluation result: 93.39614724721449\n",
      "Finished episode 118 with reward 116.07220536100468 in 32 steps - goal reached: True\n",
      "Finished episode 119 with reward 54.04976478572323 in 36 steps - goal reached: True\n",
      "Evaluation result: 115.23188573134033\n",
      "Evaluation result: 106.87480008548566\n",
      "Finished episode 120 with reward 112.22957953367874 in 30 steps - goal reached: True\n",
      "Evaluation result: 93.87051305624495\n",
      "Evaluation result: 110.08968837628235\n",
      "Finished episode 121 with reward 116.62663926127415 in 30 steps - goal reached: True\n",
      "Finished episode 122 with reward 113.63961780684852 in 28 steps - goal reached: True\n",
      "Evaluation result: 92.08834459829508\n",
      "Evaluation result: 110.94644263359334\n",
      "Finished episode 123 with reward 118.14974597869423 in 30 steps - goal reached: True\n",
      "Finished episode 124 with reward 119.2788090511503 in 27 steps - goal reached: True\n",
      "Evaluation result: 113.69971659358664\n",
      "Evaluation result: 109.24294208181868\n",
      "Finished episode 125 with reward 91.52597441786679 in 41 steps - goal reached: False\n",
      "Evaluation result: 111.22765761509255\n",
      "Evaluation result: 111.22765761509255\n",
      "Finished episode 126 with reward 114.41695310320294 in 32 steps - goal reached: True\n",
      "Finished episode 127 with reward 116.84684773007191 in 28 steps - goal reached: True\n",
      "Evaluation result: 69.87973037924438\n",
      "Evaluation result: 115.93800111991455\n",
      "Finished episode 128 with reward 121.60709604692279 in 27 steps - goal reached: True\n",
      "Finished episode 129 with reward 115.35700366428615 in 28 steps - goal reached: True\n",
      "Evaluation result: 114.52124209427005\n",
      "Evaluation result: 114.52124209427005\n",
      "Finished episode 130 with reward 116.13303018365464 in 26 steps - goal reached: True\n",
      "Evaluation result: 107.73932642392283\n",
      "Evaluation result: 99.5535924268357\n",
      "Finished episode 131 with reward 115.5421108951452 in 30 steps - goal reached: True\n",
      "Finished episode 132 with reward 108.46232867708326 in 32 steps - goal reached: True\n",
      "Evaluation result: 112.18712354218054\n",
      "Evaluation result: 112.18712354218054\n",
      "Finished episode 133 with reward 122.86700138428026 in 27 steps - goal reached: True\n",
      "Finished episode 134 with reward 116.93586903071683 in 28 steps - goal reached: True\n",
      "Evaluation result: 111.55601352623746\n",
      "Evaluation result: 111.55601352623746\n",
      "Finished episode 135 with reward 116.15571942417364 in 28 steps - goal reached: True\n",
      "Finished episode 136 with reward 119.64641842587734 in 29 steps - goal reached: True\n",
      "Evaluation result: 122.22193739878202\n",
      "Evaluation result: 122.22193739878202\n",
      "Finished episode 137 with reward 116.62713010119596 in 28 steps - goal reached: True\n",
      "Evaluation result: 108.38900963448086\n",
      "Finished episode 138 with reward 125.38925630661585 in 24 steps - goal reached: True\n",
      "Finished episode 139 with reward 111.14021185947468 in 32 steps - goal reached: True\n",
      "Evaluation result: 114.49293146325549\n",
      "Evaluation result: 114.49293146325549\n",
      "Finished episode 140 with reward 125.15356182129993 in 25 steps - goal reached: True\n",
      "Finished episode 141 with reward 125.49435117276411 in 24 steps - goal reached: True\n",
      "Evaluation result: 94.04896116495942\n",
      "Evaluation result: 122.23460511923317\n",
      "Finished episode 142 with reward 123.81974932157134 in 25 steps - goal reached: True\n",
      "Finished episode 143 with reward 120.0382916080821 in 23 steps - goal reached: True\n",
      "Evaluation result: 116.08494349537918\n",
      "Evaluation result: 116.08494349537918\n",
      "Finished episode 144 with reward 115.06594911288238 in 32 steps - goal reached: True\n",
      "Finished episode 145 with reward 122.71216830994642 in 23 steps - goal reached: True\n",
      "Evaluation result: 106.12848725683277\n",
      "Evaluation result: 126.21478232721337\n",
      "Finished episode 146 with reward 118.45898604517303 in 30 steps - goal reached: True\n",
      "Finished episode 147 with reward 113.66642914026966 in 31 steps - goal reached: True\n",
      "Evaluation result: 97.53317187121489\n",
      "Evaluation result: 86.93690565550358\n",
      "Finished episode 148 with reward 120.73905625567197 in 28 steps - goal reached: True\n",
      "Evaluation result: 115.4139424143335\n",
      "Finished episode 149 with reward 120.10285321008256 in 27 steps - goal reached: True\n",
      "Finished episode 150 with reward 108.04583719377864 in 35 steps - goal reached: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m         action \u001B[38;5;241m=\u001B[39m (det_action \u001B[38;5;241m+\u001B[39m noise)\u001B[38;5;241m.\u001B[39mclip(\u001B[38;5;241m-\u001B[39mtd3_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_action\u001B[39m\u001B[38;5;124m'\u001B[39m], td3_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_action\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m training_timesteps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m td3_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_after_timesteps\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m---> 44\u001B[0m         \u001B[43mtd3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtd3_config\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     last_td3_action \u001B[38;5;241m=\u001B[39m action\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Or restrictor\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/examples/agents/td3.py:117\u001B[0m, in \u001B[0;36mTD3.train\u001B[0;34m(self, replay_buffer, batch_size)\u001B[0m\n\u001B[1;32m    112\u001B[0m next_action \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    113\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor_target(next_state) \u001B[38;5;241m+\u001B[39m noise\n\u001B[1;32m    114\u001B[0m )\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_action, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_action)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# Compute the target Q value\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m target_Q1, target_Q2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcritic_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_action\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m target_Q \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmin(target_Q1, target_Q2)\n\u001B[1;32m    119\u001B[0m target_Q \u001B[38;5;241m=\u001B[39m reward \u001B[38;5;241m+\u001B[39m not_done \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscount \u001B[38;5;241m*\u001B[39m target_Q\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/test2/hicss-2024/examples/agents/td3.py:50\u001B[0m, in \u001B[0;36mCritic.forward\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m     47\u001B[0m q1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml3(q1)\n\u001B[1;32m     49\u001B[0m q2 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml4(sa))\n\u001B[0;32m---> 50\u001B[0m q2 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ml5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq2\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     51\u001B[0m q2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml6(q2)\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m q1, q2\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-mm/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "episode_num = 0\n",
    "training_timesteps = 0\n",
    "sample_from_restricted_space = True\n",
    "\n",
    "while training_timesteps < total_timesteps:\n",
    "    restricted_environment.reset()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "    observation = None\n",
    "    action = None\n",
    "    last_td3_action = None\n",
    "\n",
    "    for agent in restricted_environment.agent_iter():\n",
    "        next_observation, reward, termination, truncation, info = restricted_environment.last()\n",
    "\n",
    "        # Turn of the agent\n",
    "        if agent == 'agent_0':\n",
    "            episode_reward += reward\n",
    "            episode_timesteps += 1\n",
    "\n",
    "            flattened_next_observation = next_observation['observation']\n",
    "\n",
    "            if episode_timesteps > 1:\n",
    "                replay_buffer.add(observation,\n",
    "                                  last_td3_action,\n",
    "                                  flattened_next_observation,\n",
    "                                  reward,\n",
    "                                  termination or truncation)\n",
    "            observation = flattened_next_observation\n",
    "\n",
    "            training_timesteps += 1\n",
    "            if training_timesteps < td3_config['train_after_timesteps']:\n",
    "                if sample_from_restricted_space:\n",
    "                    action = next_observation['restriction'].sample()\n",
    "                else:\n",
    "                    action = np.random.uniform(-110.0, 110.0, (1,))\n",
    "            else:\n",
    "                det_action = td3.select_action(observation)\n",
    "                noise = np.random.normal(0, td3_config['max_action'] * td3_config['exploration_noise'], size=td3_config['action_dim'])\n",
    "                action = (det_action + noise).clip(-td3_config['max_action'], td3_config['max_action'])\n",
    "\n",
    "            if training_timesteps >= td3_config['train_after_timesteps']:\n",
    "                td3.train(replay_buffer, td3_config['batch_size'])\n",
    "            last_td3_action = action\n",
    "        # Or restrictor\n",
    "        else:\n",
    "            action = restrictor.act(next_observation)\n",
    "\n",
    "        # None action if episode is done\n",
    "        if termination or truncation:\n",
    "            # print(f'{action} for {next_observation[\"restriction\"]}, {restricted_environment.env.agent.x, restricted_environment.env.agent.y}')\n",
    "            action = None\n",
    "\n",
    "        restricted_environment.step(action)\n",
    "\n",
    "        if training_timesteps % 50 == 0:\n",
    "            print(f'Evaluation result: {evaluate(td3, restrictor)}')\n",
    "\n",
    "    print(f'Finished episode {episode_num} with reward {episode_reward} in {episode_timesteps} steps - goal reached: {termination}')\n",
    "\n",
    "restricted_environment.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
