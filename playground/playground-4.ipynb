{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.utils import flatten\n",
    "from src.wrapper import RestrictionWrapper\n",
    "from examples.agents.td3 import TD3\n",
    "from examples.envs.navigation import NavigationEnvironment\n",
    "from examples.restrictors.navigation_restrictor import NavigationRestrictor\n",
    "from examples.utils import ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'HEIGHT': 15.0,\n",
    "    'WIDTH': 15.0,\n",
    "    'STEPS_PER_EPISODE': 40,\n",
    "    'ACTION_RANGE': 220,\n",
    "    'DT': 1.0,\n",
    "    'TIMESTEP_PENALTY_COEFFICIENT': 0.05,\n",
    "    'REWARD_COLLISION': -1.0,\n",
    "    'REWARD_GOAL': 5.0,\n",
    "    'REWARD_COEFFICIENT': 5.0,\n",
    "    'AGENT_RADIUS': 0.5,\n",
    "    'AGENT_PERSPECTIVE': 90,\n",
    "    'AGENT_STEP_SIZE': 1.0,\n",
    "    'AGENT_X': 1.5,\n",
    "    'AGENT_Y': 1.5,\n",
    "    'GOAL_RADIUS': 1.0,\n",
    "    'GOAL_X': 12.0,\n",
    "    'GOAL_y': 12.0\n",
    "}\n",
    "restrictor = NavigationRestrictor(0, [[5.0, 0.0], [0.0, 5.0]], 1.0, 0.2, 0.5, 50, 8, -110.0, 110.0)\n",
    "environment = NavigationEnvironment(env_config)\n",
    "\n",
    "restricted_environment = RestrictionWrapper(environment, restrictor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "td3_config = {\n",
    "    'state_dim': 22,\n",
    "    'action_dim': 1,\n",
    "    'max_action': 110.0,\n",
    "    'discount': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'policy_noise': 0.2,\n",
    "    'noise_clip:': 0.5,\n",
    "    'policy_freq': 2,\n",
    "    'exploration_noise': 0.05,\n",
    "    'batch_size': 256,\n",
    "    'train_after_timesteps': 2000,\n",
    "    'learning_rate_actor': 1e-5,\n",
    "    'learning_rate_critic': 1e-5\n",
    "}\n",
    "\n",
    "total_timesteps = 50000\n",
    "td3 = TD3(**td3_config)\n",
    "replay_buffer = ReplayBuffer(state_dim=td3_config['state_dim'], action_dim=td3_config['action_dim'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 with reward -5.540116347695885\n",
      "Finished episode 2 with reward 0.136429468781885\n",
      "Finished episode 3 with reward -4.6367047009087905\n",
      "Finished episode 4 with reward -3.35873181805932\n",
      "Finished episode 5 with reward -5.02028808058135\n",
      "Finished episode 6 with reward 4.807869526307469\n",
      "Finished episode 7 with reward -1.41570669758535\n",
      "Finished episode 8 with reward 7.910575588219789\n",
      "Finished episode 9 with reward 14.34745782524136\n",
      "Finished episode 10 with reward 0.8409076993986\n",
      "Finished episode 11 with reward 4.8931458339053\n",
      "Finished episode 12 with reward 13.732347654674715\n",
      "Finished episode 13 with reward -2.93021658550393\n",
      "Finished episode 14 with reward 5.974999307724801\n",
      "Finished episode 15 with reward -5.01428928047424\n",
      "Finished episode 16 with reward 5.1952677421321365\n",
      "Finished episode 17 with reward -6.6107308745175395\n",
      "Finished episode 18 with reward 0.8966686065987299\n",
      "Finished episode 19 with reward -2.7616962254593798\n",
      "Finished episode 20 with reward 0.8777850908455651\n",
      "Finished episode 21 with reward -0.96470690370411\n",
      "Finished episode 22 with reward 3.4359735660711666\n",
      "Finished episode 23 with reward 4.302944883193666\n",
      "Finished episode 24 with reward -0.8171791809544611\n",
      "Finished episode 25 with reward -5.01931594310798\n",
      "Finished episode 26 with reward 5.063011427041751\n",
      "Finished episode 27 with reward 0.7668831508001599\n",
      "Finished episode 28 with reward 7.183652951927515\n",
      "Finished episode 29 with reward -1.0462030490529\n",
      "Finished episode 30 with reward 1.2338334838041405\n",
      "Finished episode 31 with reward -5.30950073257394\n",
      "Finished episode 32 with reward 55.28288215266498\n",
      "Finished episode 33 with reward 2.0167148213760253\n",
      "Finished episode 34 with reward 2.0669890448865\n",
      "Finished episode 35 with reward -5.0656764798127\n",
      "Finished episode 36 with reward -3.888679577839857\n",
      "Finished episode 37 with reward 13.602143465994217\n",
      "Finished episode 38 with reward -2.249713745077175\n",
      "Finished episode 39 with reward 2.365185140896851\n",
      "Finished episode 40 with reward 3.1733140728624853\n",
      "Finished episode 41 with reward -4.0710828914043\n",
      "Finished episode 42 with reward -4.155306696704267\n",
      "Finished episode 43 with reward -3.427845848526325\n",
      "Finished episode 44 with reward -0.5259701442237557\n",
      "Finished episode 45 with reward 1.2851532413428801\n",
      "Finished episode 46 with reward 1.3849336058341901\n",
      "Finished episode 47 with reward 19.595406852159677\n",
      "Finished episode 48 with reward -4.961372002330175\n",
      "Finished episode 49 with reward 0.6200200371668501\n",
      "Finished episode 50 with reward 2.05825278217415\n",
      "Finished episode 51 with reward 3.790555649275375\n",
      "Finished episode 52 with reward 3.003010914719175\n",
      "Finished episode 53 with reward 7.054940237793101\n",
      "Finished episode 54 with reward 58.80761454668083\n",
      "Finished episode 55 with reward 1.5777828002129648\n",
      "Finished episode 56 with reward 1.1299810407041249\n",
      "Finished episode 57 with reward 5.66683643816586\n",
      "Finished episode 58 with reward 27.93622892545443\n",
      "Finished episode 59 with reward 1.6560147382900898\n",
      "Finished episode 60 with reward -3.805409727123165\n",
      "Finished episode 61 with reward 7.86351012648127\n",
      "Finished episode 62 with reward 7.534994837250315\n",
      "Finished episode 63 with reward 2.000666404187605\n",
      "Finished episode 64 with reward -12.10079611731609\n",
      "Finished episode 65 with reward 2.03986673634858\n",
      "Finished episode 66 with reward -4.9419341402783505\n",
      "Finished episode 67 with reward 18.771345589893237\n",
      "Finished episode 68 with reward 7.463283555698531\n",
      "Finished episode 69 with reward 15.989986272675665\n",
      "Finished episode 70 with reward 3.89924736938576\n",
      "Finished episode 71 with reward -5.336352004433035\n",
      "Finished episode 72 with reward -5.33432665190489\n",
      "Finished episode 73 with reward -5.49322585985306\n",
      "Finished episode 74 with reward 2.096291031950736\n",
      "Finished episode 75 with reward -0.4335930759829103\n",
      "Finished episode 76 with reward -2.101678884260685\n",
      "Finished episode 77 with reward -10.305116706612885\n",
      "Finished episode 78 with reward 0.6773341991534005\n",
      "Finished episode 79 with reward 2.6870254522569\n",
      "Finished episode 80 with reward -2.84138367037531\n",
      "Finished episode 81 with reward -2.25935004054356\n",
      "Finished episode 82 with reward 2.500240641287715\n",
      "Finished episode 83 with reward 8.185890122822709\n",
      "Finished episode 84 with reward -5.746989538760703\n",
      "Finished episode 85 with reward -4.630529031443305\n",
      "Finished episode 86 with reward 1.21748145820573\n",
      "Finished episode 87 with reward -1.49766523526178\n",
      "Finished episode 88 with reward 8.40223570151943\n",
      "Finished episode 89 with reward 12.07921173507983\n",
      "Finished episode 90 with reward -5.367435277847364\n",
      "Finished episode 91 with reward -5.11566915638587\n",
      "Finished episode 92 with reward 0.4628429858367\n",
      "Finished episode 93 with reward 11.482511873320934\n",
      "Finished episode 94 with reward -4.6128882281893855\n",
      "Finished episode 95 with reward -17.151137001848525\n",
      "Finished episode 96 with reward -28.089257339759584\n",
      "Finished episode 97 with reward -3.6036385407957\n",
      "Finished episode 98 with reward 3.0776939928970997\n",
      "Finished episode 99 with reward 2.05849648444247\n",
      "Finished episode 100 with reward 0.532576264749075\n",
      "Finished episode 101 with reward 0.85337551166909\n",
      "Finished episode 102 with reward 1.5335282025687103\n",
      "Finished episode 103 with reward -1.1135413548506201\n",
      "Finished episode 104 with reward -3.2298020983838507\n",
      "Finished episode 105 with reward 1.5210892313433049\n",
      "Finished episode 106 with reward 0.66457668685398\n",
      "Finished episode 107 with reward 27.302352495566087\n",
      "Finished episode 108 with reward -1.2115234251154394\n",
      "Finished episode 109 with reward 4.67755578190532\n",
      "Finished episode 110 with reward 12.229739790725466\n",
      "Finished episode 111 with reward 8.295818427217142\n",
      "Finished episode 112 with reward 1.51304263913344\n",
      "Finished episode 113 with reward 7.983466313954452\n",
      "Finished episode 114 with reward 3.1813587129931493\n",
      "Finished episode 115 with reward -4.956063293617705\n",
      "Finished episode 116 with reward -24.64616492818062\n",
      "Finished episode 117 with reward 5.51023842358622\n",
      "Finished episode 118 with reward 1.495758793429435\n",
      "Finished episode 119 with reward 1.4791105415739998\n",
      "Finished episode 120 with reward -1.9226869479800648\n",
      "Finished episode 121 with reward -0.8662381015742899\n",
      "Finished episode 122 with reward 7.24276292322617\n",
      "Finished episode 123 with reward 3.849034181710266\n",
      "Finished episode 124 with reward -1.968901968500827\n",
      "Finished episode 125 with reward 1.8787180503997951\n",
      "Finished episode 126 with reward -4.333055964377101\n",
      "Finished episode 127 with reward -5.436196655245715\n",
      "Finished episode 128 with reward 0.1196883405326199\n",
      "Finished episode 129 with reward 0.6603380875315006\n",
      "Finished episode 130 with reward 14.267046897631033\n",
      "Finished episode 131 with reward -4.7775606582140995\n",
      "Finished episode 132 with reward 0.94945774936481\n",
      "Finished episode 133 with reward -6.0642594292787395\n",
      "Finished episode 134 with reward 3.0619093303611447\n",
      "Finished episode 135 with reward -5.12898197118296\n",
      "Finished episode 136 with reward 2.840381719391764\n",
      "Finished episode 137 with reward -8.03769608721501\n",
      "Finished episode 138 with reward -1.8772512830937096\n",
      "Finished episode 139 with reward 9.367929989327763\n",
      "Finished episode 140 with reward 2.26600459961778\n",
      "Finished episode 141 with reward -7.002226463666675\n",
      "Finished episode 142 with reward -3.691514636013775\n",
      "Finished episode 143 with reward -2.0369360323131653\n",
      "Finished episode 144 with reward -2.38802263848991\n",
      "Finished episode 145 with reward 1.8979063314037452\n",
      "Finished episode 146 with reward -2.48142410492138\n",
      "Finished episode 147 with reward 3.113806078732974\n",
      "Finished episode 148 with reward -2.2957712511348003\n",
      "Finished episode 149 with reward -3.2210479920341406\n",
      "Finished episode 150 with reward -4.4765854643164005\n",
      "Finished episode 151 with reward -5.534416658667685\n",
      "Finished episode 152 with reward 1.7662505837598497\n",
      "Finished episode 153 with reward -20.993204397827913\n",
      "Finished episode 154 with reward 10.499955155893936\n",
      "Finished episode 155 with reward -4.0294525846671245\n",
      "Finished episode 156 with reward 6.7779378491059585\n",
      "Finished episode 157 with reward 8.054777919976095\n",
      "Finished episode 158 with reward 2.498520417033585\n",
      "Finished episode 159 with reward 5.899218272874004\n",
      "Finished episode 160 with reward 0.791289942538475\n",
      "Finished episode 161 with reward 1.8785305485041048\n",
      "Finished episode 162 with reward 3.15117527628425\n",
      "Finished episode 163 with reward 1.1448925153904241\n",
      "Finished episode 164 with reward 0.5400551007934902\n",
      "Finished episode 165 with reward 1.8430438705543004\n",
      "Finished episode 166 with reward -19.426290446681907\n",
      "Finished episode 167 with reward 9.258593788577782\n",
      "Finished episode 168 with reward -5.343394445268115\n",
      "Finished episode 169 with reward -0.73824023286473\n",
      "Finished episode 170 with reward -2.15664117483937\n",
      "Finished episode 171 with reward -5.17854871469867\n",
      "Finished episode 172 with reward -4.5970680796220655\n",
      "Finished episode 173 with reward -16.844702258212163\n",
      "Finished episode 174 with reward -5.354447358432275\n",
      "Finished episode 175 with reward 0.21829189146201\n",
      "Finished episode 176 with reward 1.46965447705364\n",
      "Finished episode 177 with reward 13.56891738709721\n",
      "Finished episode 178 with reward 0.5799460377480097\n",
      "Finished episode 179 with reward 4.614284999188671\n",
      "Finished episode 180 with reward -1.09231857872284\n",
      "Finished episode 181 with reward -3.637231996782685\n",
      "Finished episode 182 with reward 0.1361735989371704\n",
      "Finished episode 183 with reward 10.30229443156566\n",
      "Finished episode 184 with reward 3.13050224032793\n",
      "Finished episode 185 with reward 3.2918727972876347\n",
      "Finished episode 186 with reward -3.419073591289537\n",
      "Finished episode 187 with reward 47.3202676298388\n",
      "Finished episode 188 with reward -4.715975917905315\n",
      "Finished episode 189 with reward 1.3598801665718998\n",
      "Finished episode 190 with reward 1.3260223107594298\n",
      "Finished episode 191 with reward -1.59612406106698\n",
      "Finished episode 192 with reward 1.51765649060171\n",
      "Finished episode 193 with reward 3.5124553666176297\n",
      "Finished episode 194 with reward 0.8977479512188768\n",
      "Finished episode 195 with reward -1.7202275993384601\n",
      "Finished episode 196 with reward 4.736746484873125\n",
      "Finished episode 197 with reward 5.723437255901923\n",
      "Finished episode 198 with reward 1.5423752841841356\n",
      "Finished episode 199 with reward 1.2774277569572492\n",
      "Finished episode 200 with reward 9.901793723651323\n",
      "Finished episode 201 with reward 5.776412614439325\n",
      "Finished episode 202 with reward -0.794862911730425\n",
      "Finished episode 203 with reward 4.733843678743213\n",
      "Finished episode 204 with reward 6.513197489841515\n",
      "Finished episode 205 with reward -9.405426159101468\n",
      "Finished episode 206 with reward -4.40443063128342\n",
      "Finished episode 207 with reward -1.3257341814393944\n",
      "Finished episode 208 with reward 1.2342053192927103\n",
      "Finished episode 209 with reward -5.528117859206695\n",
      "Finished episode 210 with reward -13.040299410249165\n",
      "Finished episode 211 with reward -3.281487453343695\n",
      "Finished episode 212 with reward -5.06978633095105\n",
      "Finished episode 213 with reward 1.5676878116279713\n",
      "Finished episode 214 with reward 2.418588804477454\n",
      "Finished episode 215 with reward -21.03271267972161\n",
      "Finished episode 216 with reward -5.555362623128315\n",
      "Finished episode 217 with reward -4.40615926995902\n",
      "Finished episode 218 with reward 4.234031744803885\n",
      "Finished episode 219 with reward -5.35070712443247\n",
      "Finished episode 220 with reward -1.37268831757081\n",
      "Finished episode 221 with reward 2.1104810110430403\n",
      "Finished episode 222 with reward 5.212395717405135\n",
      "Finished episode 223 with reward 9.046984764028705\n",
      "Finished episode 224 with reward 0.7250779070391746\n",
      "Finished episode 225 with reward 5.598277516981325\n",
      "Finished episode 226 with reward -2.122692662111975\n",
      "Finished episode 227 with reward 1.9926339421181147\n",
      "Finished episode 228 with reward 24.003047277819586\n",
      "Finished episode 229 with reward 1.21060641855936\n",
      "Finished episode 230 with reward 55.60584398592453\n",
      "Finished episode 231 with reward 61.15185399180016\n",
      "Finished episode 232 with reward 67.10871393998545\n",
      "Finished episode 233 with reward 60.17397726712002\n",
      "Finished episode 234 with reward 63.130785875373725\n",
      "Finished episode 235 with reward 59.28870400370863\n",
      "Finished episode 236 with reward 64.71466825424716\n",
      "Finished episode 237 with reward 64.93802955998426\n",
      "Finished episode 238 with reward 59.462074347080204\n",
      "Finished episode 239 with reward 65.98810437619267\n",
      "Finished episode 240 with reward 58.72982624909373\n",
      "Finished episode 241 with reward 59.975331197923104\n",
      "Finished episode 242 with reward 62.49992504896316\n",
      "Finished episode 243 with reward 65.4824601498251\n",
      "Finished episode 244 with reward 67.00124523692745\n",
      "Finished episode 245 with reward 60.2182417533142\n",
      "Finished episode 246 with reward 61.740851969798115\n",
      "Finished episode 247 with reward 66.39542260018696\n",
      "Finished episode 248 with reward 65.84212398854143\n",
      "Finished episode 249 with reward 60.86718764559546\n",
      "Finished episode 250 with reward 62.74524805444767\n",
      "Finished episode 251 with reward 65.15654780219819\n",
      "Finished episode 252 with reward 59.103854459819985\n",
      "Finished episode 253 with reward 61.92949418804635\n",
      "Finished episode 254 with reward 64.58026039695596\n",
      "Finished episode 255 with reward 61.945755857943595\n",
      "Finished episode 256 with reward 63.720003036770855\n",
      "Finished episode 257 with reward 60.716311470197105\n",
      "Finished episode 258 with reward 60.93207268045623\n",
      "Finished episode 259 with reward 63.4550526972139\n",
      "Finished episode 260 with reward 60.925093039636\n",
      "Finished episode 261 with reward 61.85823303043411\n",
      "Finished episode 262 with reward 59.37130936663353\n",
      "Finished episode 263 with reward 63.08162134105919\n",
      "Finished episode 264 with reward 62.64898046948578\n",
      "Finished episode 265 with reward 61.15526866095273\n",
      "Finished episode 266 with reward 61.46279932896771\n",
      "Finished episode 267 with reward 58.98490181890052\n",
      "Finished episode 268 with reward 61.932459745220704\n",
      "Finished episode 269 with reward 61.6666032781805\n",
      "Finished episode 270 with reward 61.44370705758933\n",
      "Finished episode 271 with reward 58.73179169869146\n",
      "Finished episode 272 with reward 63.906011767842124\n",
      "Finished episode 273 with reward 61.977061080312524\n",
      "Finished episode 274 with reward 58.95940564820919\n",
      "Finished episode 275 with reward 65.38673559991672\n",
      "Finished episode 276 with reward 62.81061259659412\n",
      "Finished episode 277 with reward 60.09726983628097\n",
      "Finished episode 278 with reward 62.42743066165339\n",
      "Finished episode 279 with reward 62.60761263149969\n",
      "Finished episode 280 with reward 63.91687318348401\n",
      "Finished episode 281 with reward 62.86983650479986\n",
      "Finished episode 282 with reward 65.05599837668868\n",
      "Finished episode 283 with reward 63.341787064743656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 40\u001B[0m\n\u001B[1;32m     38\u001B[0m         action \u001B[38;5;241m=\u001B[39m (det_action \u001B[38;5;241m+\u001B[39m noise)\u001B[38;5;241m.\u001B[39mclip(\u001B[38;5;241m-\u001B[39mtd3_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_action\u001B[39m\u001B[38;5;124m'\u001B[39m], td3_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_action\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m training_timesteps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m td3_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_after_timesteps\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m---> 40\u001B[0m         \u001B[43mtd3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtd3_config\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m     last_td3_action \u001B[38;5;241m=\u001B[39m action\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Or restrictor\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Institute for Enterprise Systems/HiWi/hicss-2024/examples/agents/td3.py:141\u001B[0m, in \u001B[0;36mTD3.train\u001B[0;34m(self, replay_buffer, batch_size)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    140\u001B[0m actor_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m--> 141\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;66;03m# Update the frozen target models\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param, target_param \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcritic\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcritic_target\u001B[38;5;241m.\u001B[39mparameters()):\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-training/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-training/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-training/lib/python3.10/site-packages/torch/optim/adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    130\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    133\u001B[0m         group,\n\u001B[1;32m    134\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    138\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    139\u001B[0m         state_steps)\n\u001B[0;32m--> 141\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-training/lib/python3.10/site-packages/torch/optim/adam.py:281\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    279\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 281\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/hicss-2024-training/lib/python3.10/site-packages/torch/optim/adam.py:393\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    391\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m--> 393\u001B[0m \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "episode_num = 0\n",
    "training_timesteps = 0\n",
    "\n",
    "while training_timesteps < total_timesteps:\n",
    "    restricted_environment.reset()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "    observation = None\n",
    "    action = None\n",
    "    last_td3_action = None\n",
    "\n",
    "    for agent in restricted_environment.agent_iter():\n",
    "        next_observation, reward, termination, truncation, info = restricted_environment.last()\n",
    "\n",
    "        # Turn of the agent\n",
    "        if agent == 'agent_0':\n",
    "            episode_reward += reward\n",
    "            episode_timesteps += 1\n",
    "\n",
    "            flattened_next_observation = flatten(restricted_environment.observation_space('agent_0'),\n",
    "                                                 next_observation, max_len=8, raise_error=False)\n",
    "\n",
    "            if episode_timesteps > 1:\n",
    "                replay_buffer.add(observation,\n",
    "                                  last_td3_action,\n",
    "                                  flattened_next_observation,\n",
    "                                  reward,\n",
    "                                  termination or truncation)\n",
    "            observation = flattened_next_observation\n",
    "\n",
    "            training_timesteps += 1\n",
    "            if training_timesteps < td3_config['train_after_timesteps']:\n",
    "                    action = restricted_environment.action_space(agent).sample()\n",
    "            else:\n",
    "                det_action = td3.select_action(observation)\n",
    "                noise = np.random.normal(0, td3_config['max_action'] * td3_config['exploration_noise'], size=td3_config['action_dim'])\n",
    "                action = (det_action + noise).clip(-td3_config['max_action'], td3_config['max_action'])\n",
    "            if training_timesteps >= td3_config['train_after_timesteps']:\n",
    "                td3.train(replay_buffer, td3_config['batch_size'])\n",
    "            last_td3_action = action\n",
    "        # Or restrictor\n",
    "        else:\n",
    "            action = restrictor.act(next_observation)\n",
    "\n",
    "        # None action if episode is done\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "\n",
    "        restricted_environment.step(action)\n",
    "\n",
    "    print(f'Finished episode {episode_num} with reward {episode_reward}')\n",
    "\n",
    "restricted_environment.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
